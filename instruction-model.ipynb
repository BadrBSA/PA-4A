{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:19.565658900Z",
     "start_time": "2024-05-01T18:14:19.406661200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_rslora=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:19.565658900Z",
     "start_time": "2024-05-01T18:14:19.411434200Z"
    }
   },
   "id": "a807a9179fb53681"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"pszemraj/booksum-short\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:22.191335400Z",
     "start_time": "2024-05-01T18:14:19.414999600Z"
    }
   },
   "id": "7263fc51f1ae225f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        # load_in_8bit=True,\n",
    "        load_in_4bit=True,\n",
    "        # llm_int8_enable_fp32_cpu_offload=True,\n",
    "        # llm_int8_has_fp16_weight=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:22.198571400Z",
     "start_time": "2024-05-01T18:14:22.191335400Z"
    }
   },
   "id": "261dd87971875d73"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:22.201583200Z",
     "start_time": "2024-05-01T18:14:22.198571400Z"
    }
   },
   "id": "89ea41671bac7df1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95a6e7e71d5147da919c47e237c61450"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:44.587764500Z",
     "start_time": "2024-05-01T18:14:22.202580600Z"
    }
   },
   "id": "1a28c0ec89c553f3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", model_max_length=128)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:44.863608400Z",
     "start_time": "2024-05-01T18:14:44.592282400Z"
    }
   },
   "id": "23934443aa3ee2a3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:14:44.872665Z",
     "start_time": "2024-05-01T18:14:44.866139900Z"
    }
   },
   "id": "992cca422d0efe21"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "E:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] I'm glad you asked, here's a simple and classic Homemade Mayonnaise recipe for you:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup vegetable oil (canola, safflower, or any neutral-tasting oil)\n",
      "- 1 large egg yolk\n",
      "- 1 tbsp white wine vinegar or apple cider vinegar\n",
      "- 1 tbsp lemon juice\n",
      "- 1 tsp Dijon mustard\n",
      "- 1 tsp salt\n",
      "- optional: 1/2 tsp paprika (for a nice color)\n",
      "\n",
      "Instructions:\n",
      "1. In a blender or food processor, add the egg yolk, vinegar, lemon juice, mustard, and salt. Blend for about 5 seconds to combine.\n",
      "2. With the machine running, very slowly drizzle in the oil in a thin, steady stream, near the top of the blade. Be patient and keep the flow steady until all the oil has been incorporated, about 1-2 minutes.\n",
      "3. Taste the mayonnaise and add additional salt, vinegar, or lemon juice if desired.\n",
      "4. If using paprika, add it in with the last tablespoon of oil and blend just until combined.\n",
      "\n",
      "Make sure not to add the oil too quickly, or the mayonnaise might separate. Enjoy your homemade mayonnaise on sandwiches, salads, or as a dip. It will keep in an airtight container in the refrigerator for up to a week.</s>\n"
     ]
    }
   ],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:15:05.535049300Z",
     "start_time": "2024-05-01T18:14:44.871663Z"
    }
   },
   "id": "b4daafb425acfd93"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select_columns(['chapter', 'summary']).select(range(10))\n",
    "val_dataset = dataset[\"validation\"].select_columns(['chapter', 'summary']).select(range(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:15:05.600983100Z",
     "start_time": "2024-05-01T18:15:05.534048Z"
    }
   },
   "id": "b17151a120d20d86"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chapter', 'summary'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:15:05.610984500Z",
     "start_time": "2024-05-01T18:15:05.567946600Z"
    }
   },
   "id": "82ddcf8f14a29e68"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
      "        num_rows: 5912\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
      "        num_rows: 1012\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
      "        num_rows: 988\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:15:05.611984500Z",
     "start_time": "2024-05-01T18:15:05.573032100Z"
    }
   },
   "id": "67ac210affa99d5e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def encode(dataset):\n",
    "    return tokenizer(dataset[\"chapter\"], dataset[\"summary\"], truncation=False, padding=\"max_length\", max_length=128)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:18.514415200Z",
     "start_time": "2024-05-01T18:18:18.497331800Z"
    }
   },
   "id": "a21c8d680b03aa8f"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19a1d5e0ccaa4a20bfe1a8f4104e1ecd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d9fbb6cbe2a4662ac77f2a2994ef1a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_train_dataset = train_dataset.map(encode, batched=True)\n",
    "encoded_val_dataset = val_dataset.map(encode, batched=True)\n",
    "\n",
    "# encoded_train_dataset = train_dataset.map(encode)\n",
    "# encoded_val_dataset = val_dataset.map(encode)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:20.766722700Z",
     "start_time": "2024-05-01T18:18:20.531207900Z"
    }
   },
   "id": "21b6dcf401ac90a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chapter', 'summary', 'input_ids', 'attention_mask'],\n    num_rows: 10\n})"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:23.192401400Z",
     "start_time": "2024-05-01T18:18:23.186978Z"
    }
   },
   "id": "570b7e49609a049"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "5604"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_train_dataset['input_ids'][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:26.273207900Z",
     "start_time": "2024-05-01T18:18:26.242925400Z"
    }
   },
   "id": "a567360114da9c78"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_train_dataset['attention_mask'][1][:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:41.582341200Z",
     "start_time": "2024-05-01T18:18:41.556451100Z"
    }
   },
   "id": "bf1357474fb1ad28"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '', '\\n', '', '\"', 'Well', ',', 'go', 'thy', 'way', ':', 'thou', 'sh', 'alt', 'not', 'from', 'this', 'gro', 've', '\\n', '', 'T', 'ill', 'I', 'tor', 'ment', 'the', 'e', 'for', 'this', 'injury', '.\"', '\\n', '\\n', '', '_', 'M', 'id', 'sum', 'mer', 'Night', \"'\", 's', 'Dream', '._', '\\n', '\\n', '\\n', 'The', 'words', 'were', 'still', 'in', 'the', 'mouth', 'of', 'the', 'sc', 'out', ',', 'when', 'the', 'leader', 'of', 'the', '\\n', 'party', ',', 'whose', 'approaching', 'foot', 'steps', 'had', 'caught', 'the', 'vig', 'il', 'ant', 'ear', 'of', 'the', '\\n', 'Ind', 'ian', ',', 'came', 'openly', 'into', 'view', '.', 'A', 'beaten', 'path', ',', 'such', 'as', 'those', 'made', 'by', 'the', '\\n', 'period', 'ical', 'passage', 'of', 'the', 'deer', ',', 'wound', 'through', 'a', 'little', 'gl', 'en', 'at', 'no', 'great', '\\n', 'distance', ',', 'and', 'struck', 'the', 'river', 'at', 'the', 'point', 'where', 'the', 'white', 'man', 'and', 'his', '\\n', 'red', 'companions', 'had', 'posted', 'themselves', '.', 'Along', 'this', 'track', 'the', 'trav', 'ellers', ',', '\\n', 'who', 'had', 'produced', 'a', 'surprise', 'so', 'unusual', 'in', 'the', 'depth', 's', 'of', 'the', 'forest', ',', '\\n', 'adv', 'anced', 'slowly', 'towards', 'the', 'hun', 'ter', ',', 'who', 'was', 'in', 'front', 'of', 'his', 'associ', 'ates', ',', '\\n', 'in', 'read', 'iness', 'to', 'receive', 'them', '.', '\\n', '\\n', '\"', 'Who', 'comes', '?\"', 'demanded', 'the', 'sc', 'out', ',', 'throwing', 'his', 'rifle', 'care', 'lessly', 'across', '\\n', 'his', 'left', 'arm', ',', 'and', 'keeping', 'the', 'fore', 'f', 'inger', 'of', 'his', 'right', 'hand', 'on', 'the', '\\n', 'trigger', ',', 'though', 'he', 'avoided', 'all', 'appearance', 'of', 'men', 'ace', 'in', 'the', 'act', ',', '\"', 'Who', '\\n', 'comes', 'h', 'ither', ',', 'among', 'the', 'be', 'asts', 'and', 'd', 'angers', 'of', 'the', 'wild', 'erness', '?\"', '\\n', '\\n', '\"', 'Bel', 'ie', 'vers', 'in', 'religion', ',', 'and', 'friends', 'to', 'the', 'law', 'and', 'to', 'the', 'king', ',\"', '\\n', 'return', 'ed', 'he', 'who', 'rode', 'fore', 'most', '.', '\"', 'Men', 'who', 'have', 'journey', 'ed', 'since', 'the', 'rising', '\\n', 'sun', ',', 'in', 'the', 'sh', 'ades', 'of', 'this', 'forest', ',', 'without', 'n', 'our', 'ishment', ',', 'and', 'are', 'sadly', '\\n', 't', 'ired', 'of', 'their', 'way', 'f', 'aring', '.\"', '\\n', '\\n', '\"', 'You', 'are', ',', 'then', ',', 'lost', ',\"', 'interrupted', 'the', 'hun', 'ter', ',', '\"', 'and', 'have', 'found', 'how', '\\n', 'hel', 'pless', \"'\", 't', 'is', 'not', 'to', 'know', 'whether', 'to', 'take', 'the', 'right', 'hand', 'or', 'the', 'left', '?\"', '\\n', '\\n', '\"', 'Even', 'so', ';', 'suck', 'ing', 'bab', 'es', 'are', 'not', 'more', 'dependent', 'on', 'those', 'who', 'guide', 'them', '\\n', 'than', 'we', 'who', 'are', 'of', 'larger', 'growth', ',', 'and', 'who', 'may', 'now', 'be', 'said', 'to', 'possess', 'the', '\\n', 'st', 'ature', 'without', 'the', 'knowledge', 'of', 'men', '.', 'Know', 'you', 'the', 'distance', 'to', 'a', 'post', 'of', '\\n', 'the', 'crown', 'called', 'William', 'Henry', '?\"', '\\n', '\\n', '\"', 'H', 'oot', '!\"', 'shouted', 'the', 'sc', 'out', ',', 'who', 'did', 'not', 'spare', 'his', 'open', 'laughter', ',', 'though', ',', '\\n', 'inst', 'antly', 'checking', 'the', 'dangerous', 'sounds', ',', 'he', 'indul', 'ged', 'his', 'm', 'err', 'iment', 'at', '\\n', 'less', 'risk', 'of', 'being', 'over', 'he', 'ard', 'by', 'any', 'lur', 'king', 'enemies', '.', '\"', 'You', 'are', 'as', 'much', '\\n', 'off', 'the', 'scent', 'as', 'a', 'h', 'ound', 'would', 'be', ',', 'with', 'Hor', 'ican', 'at', 'wi', 'xt', 'him', 'and', 'the', 'deer', '!', '\\n', 'Will', 'iam', 'Henry', ',', 'man', '!', 'if', 'you', 'are', 'friends', 'to', 'the', 'king', ',', 'and', 'have', 'business', '\\n', 'with', 'the', 'army', ',', 'your', 'better', 'way', 'would', 'be', 'to', 'follow', 'the', 'river', 'down', 'to', '\\n', 'Ed', 'ward', ',', 'and', 'lay', 'the', 'matter', 'before', 'We', 'bb', ';', 'who', 'tar', 'ries', 'there', ',', 'instead', 'of', '\\n', 'push', 'ing', 'into', 'the', 'def', 'iles', ',', 'and', 'driving', 'this', 'sau', 'cy', 'French', 'man', 'back', 'across', '\\n', 'Ch', 'am', 'plain', ',', 'into', 'his', 'den', 'again', '.\"', '\\n', '\\n', 'Before', 'the', 'stranger', 'could', 'make', 'any', 'reply', 'to', 'this', 'unexpected', 'proposition', ',', '\\n', 'an', 'other', 'horse', 'man', 'dashed', 'the', 'bus', 'hes', 'aside', ',', 'and', 'le', 'aped', 'his', 'charg', 'er', 'into', '\\n', 'the', 'path', 'way', ',', 'in', 'front', 'of', 'his', 'companion', '.', '\\n', '\\n', '\"', 'What', ',', 'then', ',', 'may', 'be', 'our', 'distance', 'from', 'Fort', 'Edward', '?\"', 'demanded', 'a', 'new', '\\n', 'spe', 'aker', ';', '\"', 'the', 'place', 'you', 'advise', 'us', 'to', 'seek', 'we', 'left', 'this', 'morning', ',', 'and', 'our', '\\n', 'destination', 'is', 'the', 'head', 'of', 'the', 'lake', '.\"', '\\n', '\\n', '\"', 'Then', 'you', 'must', 'have', 'lost', 'your', 'eyes', 'ight', 'a', 'fore', 'losing', 'your', 'way', ',', 'for', 'the', '\\n', 'road', 'across', 'the', 'port', 'age', 'is', 'cut', 'to', 'a', 'good', 'two', 'ro', 'ds', ',', 'and', 'is', 'as', 'grand', 'a', '\\n', 'path', ',', 'I', 'calculate', ',', 'as', 'any', 'that', 'runs', 'into', 'London', ',', 'or', 'even', 'before', 'the', '\\n', 'pal', 'ace', 'of', 'the', 'king', 'himself', '.\"', '\\n', '\\n', '\"', 'We', 'will', 'not', 'dispute', 'concerning', 'the', 'excell', 'ence', 'of', 'the', 'passage', ',\"', 'returned', '\\n', 'Hey', 'ward', ',', 'smiling', ';', 'for', ',', 'as', 'the', 'reader', 'has', 'anticipated', ',', 'it', 'was', 'he', '.', '\"', 'It', 'is', '\\n', 'en', 'ough', ',', 'for', 'the', 'present', ',', 'that', 'we', 'trusted', 'to', 'an', 'Indian', 'guide', 'to', 'take', 'us', '\\n', 'by', 'a', 'near', 'er', ',', 'though', 'bl', 'inder', 'path', ',', 'and', 'that', 'we', 'are', 'de', 'ceived', 'in', 'his', '\\n', 'know', 'ledge', '.', 'In', 'plain', 'words', ',', 'we', 'know', 'not', 'where', 'we', 'are', '.\"', '\\n', '\\n', '\"', 'An', 'Indian', 'lost', 'in', 'the', 'woods', '!\"', 'said', 'the', 'sc', 'out', ',', 'shaking', 'his', 'head', '\\n', 'd', 'ou', 'b', 'ting', 'ly', ';', '\"', 'when', 'the', 'sun', 'is', 'sc', 'or', 'ching', 'the', 'tree', '-', 't', 'ops', ',', 'and', 'the', '\\n', 'water', '-', 'c', 'ourses', 'are', 'full', ';', 'when', 'the', 'm', 'oss', 'on', 'every', 'be', 'ech', 'he', 'sees', ',', 'will', 'tell', '\\n', 'him', 'in', 'which', 'quarter', 'the', 'north', 'star', 'will', 'shine', 'at', 'night', '!', 'The', 'woods', 'are', '\\n', 'full', 'of', 'deer', 'paths', 'which', 'run', 'to', 'the', 'streams', 'and', 'l', 'icks', ',', 'places', 'well', 'known', '\\n', 'to', 'everybody', ';', 'nor', 'have', 'the', 'ge', 'ese', 'done', 'their', 'flight', 'to', 'the', 'Canada', 'waters', '\\n', 'alt', 'ogether', '!', \"'\", 'T', 'is', 'strange', 'that', 'an', 'Indian', 'should', 'be', 'lost', 'at', 'wi', 'xt', 'Hor', 'ican', '\\n', 'and', 'the', 'bend', 'in', 'the', 'river', '.', 'Is', 'he', 'a', 'Moh', 'awk', '?\"', '\\n', '\\n', '\"', 'Not', 'by', 'birth', ',', 'though', 'adopted', 'in', 'that', 'tribe', ';', 'I', 'think', 'his', 'birth', 'place', 'was', '\\n', 'far', 'ther', 'north', ',', 'and', 'he', 'is', 'one', 'of', 'those', 'you', 'call', 'a', 'Hur', 'on', '.\"', '\\n', '\\n', '\"', 'H', 'ugh', '!\"', 'excl', 'aimed', 'the', 'two', 'companions', 'of', 'the', 'sc', 'out', ',', 'who', 'had', 'continued', ',', '\\n', 'until', 'this', 'part', 'of', 'the', 'dialogue', ',', 'seated', 'imm', 'ov', 'able', ',', 'and', 'apparently', '\\n', 'ind', 'ifferent', 'to', 'what', 'passed', ',', 'but', 'who', 'now', 'spr', 'ang', 'to', 'their', 'feet', 'with', 'an', '\\n', 'activity', 'and', 'interest', 'that', 'had', 'evident', 'ly', 'got', 'the', 'better', 'of', 'their', '\\n', 'res', 'erve', ',', 'by', 'surprise', '.', '\\n', '\\n', '\"', 'A', 'Hur', 'on', '!\"', 'repeated', 'the', 'st', 'ur', 'dy', 'sc', 'out', ',', 'once', 'more', 'shaking', 'his', 'head', 'in', 'open', '\\n', 'd', 'istr', 'ust', ';', '\"', 'they', 'are', 'a', 'th', 'iev', 'ish', 'race', ',', 'nor', 'do', 'I', 'care', 'by', 'whom', 'they', 'are', '\\n', 'ad', 'opt', 'ed', ';', 'you', 'can', 'never', 'make', 'anything', 'of', 'them', 'but', 'sk', 'ul', 'ks', 'and', 'vag', 'ab', 'onds', '.', '\\n', 'Since', 'you', 'trusted', 'yourself', 'to', 'the', 'care', 'of', 'one', 'of', 'that', 'nation', ',', 'I', 'only', '\\n', 'w', 'onder', 'that', 'you', 'have', 'not', 'fallen', 'in', 'with', 'more', '.\"', '\\n', '\\n', '\"', 'Of', 'that', 'there', 'is', 'little', 'danger', ',', 'since', 'William', 'Henry', 'is', 'so', 'many', 'miles', 'in', '\\n', 'our', 'front', '.', 'You', 'forget', 'that', 'I', 'have', 'told', 'you', 'our', 'guide', 'is', 'now', 'a', 'Moh', 'awk', ',', '\\n', 'and', 'that', 'he', 'serves', 'with', 'our', 'forces', 'as', 'a', 'friend', '.\"', '\\n', '\\n', '\"', 'And', 'I', 'tell', 'you', 'that', 'he', 'who', 'is', 'born', 'a', 'M', 'ingo', 'will', 'die', 'a', 'M', 'ingo', ',\"', 'returned', '\\n', 'the', 'other', ',', 'posit', 'ively', '.', '\"', 'A', 'Moh', 'awk', '!', 'No', ',', 'give', 'me', 'a', 'Del', 'aware', 'or', 'a', 'Moh', 'ican', '\\n', 'for', 'honest', 'y', ';', 'and', 'when', 'they', 'will', 'fight', ',', 'which', 'they', 'won', \"'\", 't', 'all', 'do', ',', 'having', '\\n', 's', 'uff', 'ered', 'their', 'c', 'unning', 'enemies', ',', 'the', 'Ma', 'qu', 'as', ',', 'to', 'make', 'them', 'women', '--', 'but', 'when', '\\n', 'they', 'will', 'fight', 'at', 'all', ',', 'look', 'to', 'a', 'Del', 'aware', ',', 'or', 'a', 'Moh', 'ican', ',', 'for', 'a', '\\n', 'war', 'rior', '!\"', '\\n', '\\n', '\"', 'En', 'ough', 'of', 'this', ',\"', 'said', 'Hey', 'ward', ',', 'impat', 'ient', 'ly', ';', '\"', 'I', 'wish', 'not', 'to', 'in', 'quire', 'into', '\\n', 'the', 'character', 'of', 'a', 'man', 'that', 'I', 'know', ',', 'and', 'to', 'whom', 'you', 'must', 'be', 'a', 'stranger', '.', '\\n', 'You', 'have', 'not', 'yet', 'answered', 'my', 'question', ':', 'what', 'is', 'our', 'distance', 'from', 'the', '\\n', 'main', 'army', 'at', 'Edward', '?\"', '\\n', '\\n', '\"', 'It', 'seems', 'that', 'may', 'depend', 'on', 'who', 'is', 'your', 'guide', '.', 'One', 'would', 'think', 'such', 'a', '\\n', 'hor', 'se', 'as', 'that', 'might', 'get', 'over', 'a', 'good', 'deal', 'of', 'ground', 'at', 'wi', 'xt', 'sun', '-', 'up', 'and', '\\n', 'sun', '-', 'down', '.\"', '\\n', '\\n', '\"', 'I', 'wish', 'no', 'cont', 'ention', 'of', 'idle', 'words', 'with', 'you', ',', 'friend', ',\"', 'said', 'Hey', 'ward', ',', '\\n', 'cur', 'bing', 'his', 'diss', 'atisf', 'ied', 'manner', ',', 'and', 'speaking', 'in', 'a', 'more', 'gentle', 'voice', ';', '\\n', '\"', 'if', 'you', 'will', 'tell', 'me', 'the', 'distance', 'to', 'Fort', 'Edward', ',', 'and', 'conduct', 'me', '\\n', 'th', 'ither', ',', 'your', 'labor', 'shall', 'not', 'go', 'without', 'its', 'reward', '.\"', '\\n', '\\n', '\"', 'And', 'in', 'so', 'doing', ',', 'how', 'know', 'I', 'that', 'I', 'don', \"'\", 't', 'guide', 'an', 'enemy', ',', 'and', 'a', 'spy', 'of', '\\n', 'Mont', 'cal', 'm', ',', 'to', 'the', 'works', 'of', 'the', 'army', '?', 'It', 'is', 'not', 'every', 'man', 'who', 'can', 'speak', '\\n', 'the', 'English', 'tongue', 'that', 'is', 'an', 'honest', 'subject', '.\"', '\\n', '\\n', '\"', 'If', 'you', 'serve', 'with', 'the', 'troops', ',', 'of', 'whom', 'I', 'judge', 'you', 'to', 'be', 'a', 'sc', 'out', ',', 'you', '\\n', 'should', 'know', 'of', 'such', 'a', 'reg', 'iment', 'of', 'the', 'king', 'as', 'the', '', '6', '0', 'th', '.\"', '\\n', '\\n', '\"', 'The', '', '6', '0', 'th', '!', 'you', 'can', 'tell', 'me', 'little', 'of', 'the', 'Royal', 'Americans', 'that', 'I', 'don', \"'\", 't', '\\n', 'know', ',', 'though', 'I', 'do', 'wear', 'a', 'hunting', '-', 'shirt', 'instead', 'of', 'a', 'scar', 'let', 'jacket', '.\"', '\\n', '\\n', '\"', 'Well', ',', 'then', ',', 'among', 'the', 'other', 'things', ',', 'you', 'may', 'know', 'the', 'name', 'of', 'its', '\\n', 'major', '?\"', '\\n', '\\n', '\"', 'I', 'ts', 'major', '!\"', 'interrupted', 'the', 'hun', 'ter', ',', 'elev', 'ating', 'his', 'body', 'like', 'one', 'who', 'was', '\\n', 'p', 'roud', 'of', 'his', 'trust', '.', '\"', 'If', 'there', 'is', 'a', 'man', 'in', 'the', 'country', 'who', 'knows', 'Major', '\\n', 'E', 'ff', 'ingham', ',', 'he', 'stands', 'before', 'you', '.\"', '\\n', '\\n', '\"', 'It', 'is', 'a', 'corps', 'which', 'has', 'many', 'maj', 'ors', ';', 'the', 'gentleman', 'you', 'name', 'is', 'the', '\\n', 'sen', 'ior', ',', 'but', 'I', 'speak', 'of', 'the', 'junior', 'of', 'them', 'all', ';', 'he', 'who', 'commands', 'the', '\\n', 'comp', 'an', 'ies', 'in', 'g', 'arr', 'ison', 'at', 'William', 'Henry', '.\"', '\\n', '\\n', '\"', 'Yes', ',', 'yes', ',', 'I', 'have', 'heard', 'that', 'a', 'young', 'gentleman', 'of', 'vast', 'ric', 'hes', ',', 'from', 'one', '\\n', 'of', 'the', 'provin', 'ces', 'far', 'south', ',', 'has', 'got', 'the', 'place', '.', 'He', 'is', 'over', 'young', ',', 'too', ',', 'to', '\\n', 'hold', 'such', 'rank', ',', 'and', 'to', 'be', 'put', 'above', 'men', 'whose', 'heads', 'are', 'beginning', 'to', '\\n', 'ble', 'ach', ';', 'and', 'yet', 'they', 'say', 'he', 'is', 'a', 'soldier', 'in', 'his', 'knowledge', ',', 'and', 'a', 'gall', 'ant', '\\n', 'gent', 'le', 'man', '!\"', '\\n', '\\n', '\"', 'Wh', 'atever', 'he', 'may', 'be', ',', 'or', 'however', 'he', 'may', 'be', 'qualified', 'for', 'his', 'rank', ',', 'he', 'now', '\\n', 'spe', 'aks', 'to', 'you', ',', 'and', 'of', 'course', 'can', 'be', 'no', 'enemy', 'to', 'dread', '.\"', '\\n', '\\n', 'The', 'sc', 'out', 'regarded', 'Hey', 'ward', 'in', 'surprise', ',', 'and', 'then', 'lifting', 'his', 'cap', ',', 'he', '\\n', 'ans', 'w', 'ered', ',', 'in', 'a', 'tone', 'less', 'confident', 'than', 'before', ',', 'though', 'still', 'express', 'ing', '\\n', 'd', 'oubt', ',', '--', '\\n', '\\n', '\"', 'I', 'have', 'heard', 'a', 'party', 'was', 'to', 'leave', 'the', 'enc', 'amp', 'ment', 'this', 'morning', ',', 'for', 'the', '\\n', 'l', 'ake', 'shore', '.\"', '\\n', '\\n', '\"', 'You', 'have', 'heard', 'the', 'truth', ';', 'but', 'I', 'preferred', 'a', 'near', 'er', 'route', ',', 'trust', 'ing', 'to', '\\n', 'the', 'knowledge', 'of', 'the', 'Indian', 'I', 'mentioned', '.\"', '\\n', '\\n', '\"', 'And', 'he', 'de', 'ceived', 'you', ',', 'and', 'then', 'desert', 'ed', '?\"', '\\n', '\\n', '\"', 'Ne', 'ither', ',', 'as', 'I', 'believe', ';', 'certainly', 'not', 'the', 'latter', ',', 'for', 'he', 'is', 'to', 'be', 'found', '\\n', 'in', 'the', 'rear', '.\"', '\\n', '\\n', '\"', 'I', 'should', 'like', 'to', 'look', 'at', 'the', 'creat', 'ur', \"';\", 'if', 'it', 'is', 'a', 'true', 'I', 'ro', 'qu', 'ois', 'I', 'can', '\\n', 't', 'ell', 'him', 'by', 'his', 'kn', 'av', 'ish', 'look', ',', 'and', 'by', 'his', 'paint', ',\"', 'said', 'the', 'sc', 'out', ',', '\\n', 'ste', 'pping', 'past', 'the', 'charg', 'er', 'of', 'Hey', 'ward', ',', 'and', 'entering', 'the', 'path', 'behind', 'the', '\\n', 'mare', 'of', 'the', 'singing', '-', 'master', ',', 'whose', 'fo', 'al', 'had', 'taken', 'advantage', 'of', 'the', 'halt', '\\n', 'to', 'exact', 'the', 'mater', 'nal', 'contribution', '.', 'After', 'sh', 'oving', 'aside', 'the', 'bus', 'hes', ',', 'and', '\\n', 'pro', 'ceed', 'ing', 'a', 'few', 'p', 'aces', ',', 'he', 'encountered', 'the', 'females', ',', 'who', 'await', 'ed', 'the', '\\n', 'result', 'of', 'the', 'conference', 'with', 'anxiety', ',', 'and', 'not', 'entirely', 'without', '\\n', 'app', 're', 'hens', 'ion', '.', 'Behind', 'these', ',', 'the', 'runner', 'leaned', 'against', 'a', 'tree', ',', 'where', 'he', '\\n', 'stood', 'the', 'close', 'examination', 'of', 'the', 'sc', 'out', 'with', 'an', 'air', 'unm', 'oved', ',', 'though', '\\n', 'with', 'a', 'look', 'so', 'dark', 'and', 'sav', 'age', ',', 'that', 'it', 'might', 'in', 'itself', 'ex', 'cite', 'fear', '.', '\\n', 'S', 'atisf', 'ied', 'with', 'his', 'scrut', 'iny', ',', 'the', 'hun', 'ter', 'soon', 'left', 'him', '.', 'As', 'he', 'rep', 'ass', 'ed', '\\n', 'the', 'females', ',', 'he', 'paused', 'a', 'moment', 'to', 'gaze', 'upon', 'their', 'beauty', ',', 'answering', 'to', '\\n', 'the', 'smile', 'and', 'nod', 'of', 'Alice', 'with', 'a', 'look', 'of', 'open', 'pleasure', '.', 'Then', 'ce', 'he', 'went', '\\n', 'to', 'the', 'side', 'of', 'the', 'mother', 'ly', 'animal', ',', 'and', 'spending', 'a', 'minute', 'in', 'a', 'fruit', 'less', '\\n', 'in', 'quiry', 'into', 'the', 'character', 'of', 'her', 'r', 'ider', ',', 'he', 'shook', 'his', 'head', 'and', 'returned', '\\n', 'to', 'Hey', 'ward', '.', '\\n', '\\n', '\"', 'A', 'M', 'ingo', 'is', 'a', 'M', 'ingo', ',', 'and', 'God', 'having', 'made', 'him', 'so', ',', 'neither', 'the', 'Moh', 'aw', 'ks', 'nor', '\\n', 'any', 'other', 'tribe', 'can', 'alter', 'him', ',\"', 'he', 'said', ',', 'when', 'he', 'had', 'reg', 'ained', 'his', 'former', '\\n', 'position', '.', '\"', 'If', 'we', 'were', 'alone', ',', 'and', 'you', 'would', 'leave', 'that', 'noble', 'horse', 'at', 'the', '\\n', 'mer', 'cy', 'of', 'the', 'w', 'olves', 'to', '-', 'night', ',', 'I', 'could', 'show', 'you', 'the', 'way', 'to', 'Edward', ',', '\\n', 'mys', 'elf', ',', 'within', 'an', 'hour', ',', 'for', 'it', 'lies', 'only', 'about', 'an', 'hour', \"'\", 's', 'journey', 'hence', ';', '\\n', 'but', 'with', 'such', 'ladies', 'in', 'your', 'company', \"'\", 't', 'is', 'impossible', '!\"', '\\n', '\\n', '\"', 'And', 'why', '?', 'they', 'are', 'fat', 'ig', 'ued', ',', 'but', 'they', 'are', 'quite', 'equal', 'to', 'a', 'ride', 'of', 'a', 'few', '\\n', 'more', 'miles', '.\"', '\\n', '\\n', '\"', \"'\", 'T', 'is', 'a', 'natural', 'im', 'poss', 'ibility', '!\"', 'repeated', 'the', 'sc', 'out', ';', '\"', 'I', 'wouldn', \"'\", 't', 'walk', 'a', '\\n', 'mile', 'in', 'these', 'woods', 'after', 'night', 'gets', 'into', 'them', ',', 'in', 'company', 'with', 'that', '\\n', 'runner', ',', 'for', 'the', 'best', 'rifle', 'in', 'the', 'colon', 'ies', '.', 'They', 'are', 'full', 'of', 'out', 'lying', '\\n', 'I', 'ro', 'qu', 'ois', ',', 'and', 'your', 'mong', 'rel', 'Moh', 'awk', 'knows', 'where', 'to', 'find', 'them', 'too', 'well', ',', 'to', '\\n', 'be', 'my', 'companion', '.\"', '\\n', '\\n', '\"', 'Th', 'ink', 'you', 'so', '?\"', 'said', 'Hey', 'ward', ',', 'leaning', 'forward', 'in', 'the', 'saddle', ',', 'and', '\\n', 'dro', 'pping', 'his', 'voice', 'nearly', 'to', 'a', 'whisper', ';', '\"', 'I', 'confess', 'I', 'have', 'not', 'been', '\\n', 'without', 'my', 'own', 'suspic', 'ions', ',', 'though', 'I', 'have', 'ende', 'av', 'ored', 'to', 'conce', 'al', 'them', ',', 'and', '\\n', 'aff', 'ect', 'ed', 'a', 'confidence', 'I', 'have', 'not', 'always', 'felt', ',', 'on', 'account', 'of', 'my', '\\n', 'comp', 'an', 'ions', '.', 'It', 'was', 'because', 'I', 'suspected', 'him', 'that', 'I', 'would', 'follow', 'no', '\\n', 'long', 'er', ';', 'making', 'him', ',', 'as', 'you', 'see', ',', 'follow', 'me', '.\"', '\\n', '\\n', '\"', 'I', 'knew', 'he', 'was', 'one', 'of', 'the', 'che', 'ats', 'as', 'soon', 'as', 'I', 'laid', 'eyes', 'on', 'him', '!\"', '\\n', 'return', 'ed', 'the', 'sc', 'out', ',', 'placing', 'a', 'finger', 'on', 'his', 'nose', ',', 'in', 'sign', 'of', 'caution', '.', '\\n', '\"', 'The', 'th', 'ief', 'is', 'leaning', 'against', 'the', 'foot', 'of', 'the', 'sugar', 'sap', 'ling', ',', 'that', 'you', '\\n', 'can', 'see', 'over', 'them', 'bus', 'hes', ';', 'his', 'right', 'leg', 'is', 'in', 'a', 'line', 'with', 'the', 'bark', 'of', '\\n', 'the', 'tree', ',', 'and', ',\"', 't', 'apping', 'his', 'rifle', ',', '\"', 'I', 'can', 'take', 'him', 'from', 'where', 'I', 'stand', ',', '\\n', 'between', 'the', 'an', 'kle', 'and', 'the', 'knee', ',', 'with', 'a', 'single', 'shot', ',', 'putting', 'an', 'end', 'to', '\\n', 'his', 'tr', 'amp', 'ing', 'through', 'the', 'woods', ',', 'for', 'at', 'least', 'a', 'month', 'to', 'come', '.', 'If', 'I', '\\n', 'should', 'go', 'back', 'to', 'him', ',', 'the', 'c', 'unning', 'var', 'm', 'int', 'would', 'suspect', 'something', ',', 'and', '\\n', 'be', 'dod', 'ging', 'through', 'the', 'trees', 'like', 'a', 'frightened', 'deer', '.\"', '\\n', '\\n', '\"', 'It', 'will', 'not', 'do', '.', 'He', 'may', 'be', 'innocent', ',', 'and', 'I', 'dis', 'like', 'the', 'act', '.', 'Though', ',', 'if', 'I', '\\n', 'f', 'elt', 'confident', 'of', 'his', 'tre', 'ach', 'ery', '--', '\"', '\\n', '\\n', '\"', \"'\", 'T', 'is', 'a', 'safe', 'thing', 'to', 'calculate', 'on', 'the', 'kn', 'a', 'very', 'of', 'an', 'I', 'ro', 'qu', 'ois', ',\"', 'said', 'the', '\\n', 'sc', 'out', ',', 'throwing', 'his', 'rifle', 'forward', ',', 'by', 'a', 'sort', 'of', 'instinct', 'ive', 'movement', '.', '\\n', '\\n', '\"', 'H', 'old', '!\"', 'interrupted', 'Hey', 'ward', ',', '\"', 'it', 'will', 'not', 'do', '--', 'we', 'must', 'think', 'of', 'some', '\\n', 'other', 'scheme', ';', 'and', 'yet', ',', 'I', 'have', 'much', 'reason', 'to', 'believe', 'the', 'ras', 'cal', 'has', '\\n', 'de', 'ceived', 'me', '.\"', '\\n', '\\n', 'The', 'hun', 'ter', ',', 'who', 'had', 'already', 'abandoned', 'his', 'intention', 'of', 'm', 'aim', 'ing', 'the', '\\n', 'runner', ',', 'mus', 'ed', 'a', 'moment', ',', 'and', 'then', 'made', 'a', 'gesture', ',', 'which', 'instantly', 'brought', '\\n', 'his', 'two', 'red', 'companions', 'to', 'his', 'side', '.', 'They', 'spoke', 'together', 'earn', 'estly', 'in', 'the', '\\n', 'Del', 'aware', 'language', ',', 'though', 'in', 'an', 'undert', 'one', ';', 'and', 'by', 'the', 'gest', 'ures', 'of', 'the', '\\n', 'white', 'man', ',', 'which', 'were', 'frequently', 'directed', 'towards', 'the', 'top', 'of', 'the', '\\n', 'sap', 'ling', ',', 'it', 'was', 'evident', 'he', 'pointed', 'out', 'the', 'situation', 'of', 'their', 'hidden', '\\n', 'en', 'emy', '.', 'His', 'companions', 'were', 'not', 'long', 'in', 'compre', 'hend', 'ing', 'his', 'wishes', ',', 'and', '\\n', 'lay', 'ing', 'aside', 'their', 'fire', '-', 'arms', ',', 'they', 'part', 'ed', ',', 'taking', 'opposite', 'sides', 'of', 'the', '\\n', 'path', ',', 'and', 'b', 'ury', 'ing', 'themselves', 'in', 'the', 'thick', 'et', ',', 'with', 'such', 'caut', 'ious', '\\n', 'move', 'ments', ',', 'that', 'their', 'steps', 'were', 'in', 'aud', 'ible', '.', '\\n', '\\n', '\"', 'Now', ',', 'go', 'you', 'back', ',\"', 'said', 'the', 'hun', 'ter', ',', 'speaking', 'again', 'to', 'Hey', 'ward', ',', '\"', 'and', '\\n', 'hold', 'the', 'imp', 'in', 'talk', ';', 'these', 'Moh', 'icans', 'here', 'will', 'take', 'him', 'without', 'breaking', '\\n', 'his', 'paint', '.\"', '\\n', '\\n', '\"', 'N', 'ay', ',\"', 'said', 'Hey', 'ward', ',', 'proud', 'ly', ',', '\"', 'I', 'will', 'se', 'ize', 'him', 'myself', '.\"', '\\n', '\\n', '\"', 'Hist', '!', 'what', 'could', 'you', 'do', ',', 'mounted', ',', 'against', 'an', 'Indian', 'in', 'the', 'bus', 'hes', '?\"', '\\n', '\\n', '\"', 'I', 'will', 'dis', 'mount', '.\"', '\\n', '\\n', '\"', 'And', ',', 'think', 'you', ',', 'when', 'he', 'saw', 'one', 'of', 'your', 'feet', 'out', 'of', 'the', 'stir', 'rup', ',', 'he', '\\n', 'w', 'ould', 'wait', 'for', 'the', 'other', 'to', 'be', 'free', '?', 'Who', 'ever', 'comes', 'into', 'the', 'woods', 'to', '\\n', 'de', 'al', 'with', 'the', 'nat', 'ives', ',', 'must', 'use', 'Indian', 'f', 'ash', 'ions', ',', 'if', 'he', 'would', 'wish', 'to', '\\n', 'pro', 's', 'per', 'in', 'his', 'undert', 'ak', 'ings', '.', 'Go', ',', 'then', ',', 'talk', 'openly', 'to', 'the', 'mis', 'cre', 'ant', ',', 'and', '\\n', 'se', 'em', 'to', 'believe', 'him', 'the', 'tr', 'uest', 'friend', 'you', 'have', 'on', \"'\", 'arth', '.\"', '\\n', '\\n', 'Hey', 'ward', 'prepared', 'to', 'comply', ',', 'though', 'with', 'strong', 'disgust', 'at', 'the', 'nature', 'of', '\\n', 'the', 'office', 'he', 'was', 'comp', 'elled', 'to', 'execute', '.', 'Each', 'moment', ',', 'however', ',', 'pressed', '\\n', 'up', 'on', 'him', 'a', 'conviction', 'of', 'the', 'critical', 'situation', 'in', 'which', 'he', 'had', 'suffered', '\\n', 'his', 'in', 'val', 'uable', 'trust', 'to', 'be', 'involved', 'through', 'his', 'own', 'confidence', '.', 'The', 'sun', '\\n', 'had', 'already', 'disappeared', ',', 'and', 'the', 'woods', ',', 'suddenly', 'dep', 'rived', 'of', 'his', '\\n', 'light', ',', '[', '9', ']', 'were', 'assuming', 'a', 'd', 'us', 'ky', 'h', 'ue', ',', 'which', 'keen', 'ly', 'reminded', 'him', 'that', 'the', '\\n', 'hour', 'the', 'sav', 'age', 'usually', 'chose', 'for', 'his', 'most', 'bar', 'bar', 'ous', 'and', 'rem', 'ors', 'eless', '\\n', 'acts', 'of', 'v', 'enge', 'ance', 'or', 'host', 'ility', ',', 'was', 'speed', 'ily', 'drawing', 'near', '.', 'St', 'im', 'ulated', 'by', '\\n', 'app', 're', 'hens', 'ion', ',', 'he', 'left', 'the', 'sc', 'out', ',', 'who', 'immediately', 'entered', 'into', 'a', 'loud', '\\n', 'con', 'versation', 'with', 'the', 'stranger', 'that', 'had', 'so', 'un', 'cer', 'emon', 'iously', 'en', 'listed', '\\n', 'him', 'self', 'in', 'the', 'party', 'of', 'trav', 'ellers', 'that', 'morning', '.', 'In', 'passing', 'his', 'gent', 'ler', '\\n', 'comp', 'an', 'ions', 'Hey', 'ward', 'ut', 'tered', 'a', 'few', 'words', 'of', 'encour', 'agement', ',', 'and', 'was', 'pleased', '\\n', 'to', 'find', 'that', ',', 'though', 'fat', 'ig', 'ued', 'with', 'the', 'exercise', 'of', 'the', 'day', ',', 'they', '\\n', 'appe', 'ared', 'to', 'entertain', 'no', 'suspicion', 'that', 'their', 'present', 'embarrass', 'ment', 'was', '\\n', 'other', 'than', 'the', 'result', 'of', 'accident', '.', 'G', 'iving', 'them', 'reason', 'to', 'believe', 'he', 'was', '\\n', 'mer', 'ely', 'employed', 'in', 'a', 'consultation', 'concerning', 'the', 'future', 'route', ',', 'he', '\\n', 'sp', 'ur', 'red', 'his', 'charg', 'er', ',', 'and', 'drew', 'the', 're', 'ins', 'again', ',', 'when', 'the', 'animal', 'had', '\\n', 'car', 'ried', 'him', 'within', 'a', 'few', 'yards', 'of', 'the', 'place', 'where', 'the', 'sull', 'en', 'runner', '\\n', 'st', 'ill', 'stood', ',', 'leaning', 'against', 'the', 'tree', '.', '\\n', '\\n', '\"', 'You', 'may', 'see', ',', 'Mag', 'ua', ',\"', 'he', 'said', ',', 'ende', 'avor', 'ing', 'to', 'assume', 'an', 'air', 'of', 'freedom', '\\n', 'and', 'confidence', ',', '\"', 'that', 'the', 'night', 'is', 'closing', 'around', 'us', ',', 'and', 'yet', 'we', 'are', 'no', '\\n', 'ne', 'ar', 'er', 'to', 'William', 'Henry', 'than', 'when', 'we', 'left', 'the', 'enc', 'amp', 'ment', 'of', 'We', 'bb', 'with', '\\n', 'the', 'rising', 'sun', '.', 'You', 'have', 'missed', 'the', 'way', ',', 'nor', 'have', 'I', 'been', 'more', 'fortunate', '.', '\\n', 'But', ',', 'happily', 'we', 'have', 'fallen', 'in', 'with', 'a', 'hun', 'ter', ',', 'he', 'whom', 'you', 'hear', 'talking', '\\n', 'to', 'the', 'singer', ',', 'that', 'is', 'acqu', 'ainted', 'with', 'the', 'deer', '-', 'paths', 'and', 'by', '-', 'ways', 'of', 'the', '\\n', 'wood', 's', ',', 'and', 'who', 'promises', 'to', 'lead', 'us', 'to', 'a', 'place', 'where', 'we', 'may', 'rest', 'secure', 'ly', '\\n', 't', 'ill', 'the', 'morning', '.\"', '\\n', '\\n', 'The', 'Indian', 'riv', 'eted', 'his', 'gl', 'owing', 'eyes', 'on', 'Hey', 'ward', 'as', 'he', 'asked', ',', 'in', 'his', '\\n', 'imper', 'fect', 'English', ',', '\"', 'Is', 'he', 'alone', '?\"', '\\n', '\\n', '\"', 'Al', 'one', '!\"', 'hes', 'itating', 'ly', 'answered', 'Hey', 'ward', 'to', 'whom', 'de', 'ception', 'was', 'too', 'new', 'to', '\\n', 'be', 'assumed', 'without', 'embarrass', 'ment', '.', '\"', 'O', '!', 'not', 'alone', ',', 'surely', ',', 'Mag', 'ua', ',', 'for', 'you', '\\n', 'know', 'that', 'we', 'are', 'with', 'him', '.\"', '\\n', '\\n', '\"', 'Then', 'Le', 'Ren', 'ard', 'Sub', 'til', 'will', 'go', ',\"', 'returned', 'the', 'runner', ',', 'cool', 'ly', 'raising', 'his', '\\n', 'l', 'ittle', 'wallet', 'from', 'the', 'place', 'where', 'it', 'had', 'l', 'ain', 'at', 'his', 'feet', ';', '\"', 'and', 'the', '\\n', 'p', 'ale', '-', 'faces', 'will', 'see', 'none', 'but', 'their', 'own', 'color', '.\"', '\\n', '\\n', '\"', 'Go', '!', 'Wh', 'om', 'call', 'you', 'Le', 'Ren', 'ard', '?\"', '\\n', '\\n', '\"', \"'\", 'T', 'is', 'the', 'name', 'his', 'Canada', 'f', 'athers', 'have', 'given', 'to', 'Mag', 'ua', ',\"', 'returned', 'the', '\\n', 'runner', ',', 'with', 'an', 'air', 'that', 'manifest', 'ed', 'his', 'pride', 'at', 'the', 'distinction', '.', '\"', 'N', 'ight', '\\n', 'is', 'the', 'same', 'as', 'day', 'to', 'Le', 'Sub', 'til', ',', 'when', 'Mun', 'ro', 'wait', 's', 'for', 'him', '.\"', '\\n', '\\n', '\"', 'And', 'what', 'account', 'will', 'Le', 'Ren', 'ard', 'give', 'the', 'chief', 'of', 'William', 'Henry', '\\n', 'con', 'cer', 'ning', 'his', 'daughters', '?', 'Will', 'he', 'dare', 'to', 'tell', 'the', 'hot', '-', 'blo', 'oded', 'Sc', 'ots', 'man', '\\n', 'that', 'his', 'children', 'are', 'left', 'without', 'a', 'guide', ',', 'though', 'Mag', 'ua', 'promised', 'to', 'be', '\\n', 'one', '?\"', '\\n', '\\n', '\"', 'Th', 'ough', 'the', 'gray', 'head', 'has', 'a', 'loud', 'voice', ',', 'and', 'a', 'long', 'arm', ',', 'Le', 'Ren', 'ard', 'will', '\\n', 'not', 'hear', 'him', ',', 'or', 'feel', 'him', ',', 'in', 'the', 'woods', '.\"', '\\n', '\\n', '\"', 'But', 'what', 'will', 'the', 'Moh', 'aw', 'ks', 'say', '?', 'They', 'will', 'make', 'him', 'p', 'ett', 'ico', 'ats', ',', 'and', 'bid', '\\n', 'him', 'stay', 'in', 'the', 'w', 'ig', 'w', 'am', 'with', 'the', 'women', ',', 'for', 'he', 'is', 'no', 'longer', 'to', 'be', 'trusted', '\\n', 'with', 'the', 'business', 'of', 'a', 'man', '.\"', '\\n', '\\n', '\"', 'Le', 'Sub', 'til', 'knows', 'the', 'path', 'to', 'the', 'great', 'l', 'akes', ',', 'and', 'he', 'can', 'find', 'the', 'bones', '\\n', 'of', 'his', 'f', 'athers', ',\"', 'was', 'the', 'answer', 'of', 'the', 'unm', 'oved', 'runner', '.', '\\n', '\\n', '\"', 'En', 'ough', ',', 'Mag', 'ua', ',\"', 'said', 'Hey', 'ward', ';', '\"', 'are', 'we', 'not', 'friends', '?', 'Why', 'should', 'there', 'be', '\\n', 'bit', 'ter', 'words', 'between', 'us', '?', 'Mun', 'ro', 'has', 'promised', 'you', 'a', 'gift', 'for', 'your', 'services', '\\n', 'when', 'performed', ',', 'and', 'I', 'shall', 'be', 'your', 'debt', 'or', 'for', 'another', '.', 'Rest', 'your', 'we', 'ary', '\\n', 'lim', 'bs', ',', 'then', ',', 'and', 'open', 'your', 'wallet', 'to', 'eat', '.', 'We', 'have', 'a', 'few', 'moments', 'to', '\\n', 'sp', 'are', ';', 'let', 'us', 'not', 'waste', 'them', 'in', 'talk', 'like', 'wr', 'ang', 'ling', 'women', '.', 'When', 'the', '\\n', 'lad', 'ies', 'are', 'refres', 'hed', 'we', 'will', 'proceed', '.\"', '\\n', '\\n', '\"', 'The', 'pale', '-', 'faces', 'make', 'themselves', 'dogs', 'to', 'their', 'women', ',\"', 'muttered', 'the', '\\n', 'Ind', 'ian', ',', 'in', 'his', 'native', 'language', ',', '\"', 'and', 'when', 'they', 'want', 'to', 'eat', ',', 'their', '\\n', 'war', 'riors', 'must', 'lay', 'aside', 'the', 'to', 'ma', 'h', 'awk', 'to', 'feed', 'their', 'la', 'z', 'iness', '.\"', '\\n', '\\n', '\"', 'What', 'say', 'you', ',', 'Ren', 'ard', '?\"', '\\n', '\\n', '\"', 'Le', 'Sub', 'til', 'says', 'it', 'is', 'good', '.\"', '\\n', '\\n', 'The', 'Indian', 'then', 'fast', 'ened', 'his', 'eyes', 'keen', 'ly', 'on', 'the', 'open', 'count', 'en', 'ance', 'of', '\\n', 'Hey', 'ward', ',', 'but', 'meeting', 'his', 'glance', ',', 'he', 'turned', 'them', 'quickly', 'away', ',', 'and', '\\n', 'se', 'ating', 'himself', 'deliberately', 'on', 'the', 'ground', ',', 'he', 'drew', 'forth', 'the', 'rem', 'nant', 'of', '\\n', 'some', 'former', 'rep', 'ast', ',', 'and', 'began', 'to', 'eat', ',', 'though', 'not', 'without', 'first', 'b', 'ending', '\\n', 'his', 'looks', 'slowly', 'and', 'caut', 'iously', 'around', 'him', '.', '\\n', '\\n', '\"', 'This', 'is', 'well', ',\"', 'continued', 'Hey', 'ward', ';', '\"', 'and', 'Le', 'Ren', 'ard', 'will', 'have', 'strength', 'and', '\\n', 's', 'ight', 'to', 'find', 'the', 'path', 'in', 'the', 'morning', ';\"', 'he', 'paused', ',', 'for', 'sounds', 'like', 'the', '\\n', 'sn', 'apping', 'of', 'a', 'dried', 'stick', ',', 'and', 'the', 'rust', 'ling', 'of', 'leaves', ',', 'rose', 'from', 'the', '\\n', 'adj', 'acent', 'bus', 'hes', ',', 'but', 'rec', 'ol', 'lect', 'ing', 'himself', 'instantly', ',', 'he', 'continued', ',', '--', '\"', 'we', '\\n', 'must', 'be', 'moving', 'before', 'the', 'sun', 'is', 'seen', ',', 'or', 'Mont', 'cal', 'm', 'may', 'lie', 'in', 'our', 'path', ',', '\\n', 'and', 'shut', 'us', 'out', 'from', 'the', 'fort', 'ress', '.\"', '\\n', '\\n', 'The', 'hand', 'of', 'Mag', 'ua', 'dropped', 'from', 'his', 'mouth', 'to', 'his', 'side', ',', 'and', 'though', 'his', '\\n', 'ey', 'es', 'were', 'fast', 'ened', 'on', 'the', 'ground', ',', 'his', 'head', 'was', 'turned', 'aside', ',', 'his', '\\n', 'nost', 'r', 'ils', 'expanded', ',', 'and', 'his', 'ears', 'seemed', 'even', 'to', 'stand', 'more', 'ere', 'ct', 'than', '\\n', 'us', 'ual', ',', 'giving', 'to', 'him', 'the', 'appearance', 'of', 'a', 'statue', 'that', 'was', 'made', 'to', '\\n', 'rep', 'resent', 'intense', 'attention', '.', '\\n', '\\n', 'Hey', 'ward', ',', 'who', 'watched', 'his', 'movements', 'with', 'a', 'vig', 'il', 'ant', 'eye', ',', 'care', 'lessly', '\\n', 'extr', 'icated', 'one', 'of', 'his', 'feet', 'from', 'the', 'stir', 'rup', ',', 'while', 'he', 'passed', 'a', 'hand', '\\n', 't', 'ow', 'ards', 'the', 'bear', '-', 'skin', 'covering', 'of', 'his', 'hol', 'sters', '.', 'Every', 'effort', 'to', 'detect', '\\n', 'the', 'point', 'most', 'regarded', 'by', 'the', 'runner', 'was', 'completely', 'frustrated', 'by', 'the', '\\n', 't', 'rem', 'ulous', 'gl', 'ances', 'of', 'his', 'org', 'ans', ',', 'which', 'seemed', 'not', 'to', 'rest', 'a', 'single', '\\n', 'inst', 'ant', 'on', 'any', 'particular', 'object', ',', 'and', 'which', ',', 'at', 'the', 'same', 'time', ',', 'could', 'be', '\\n', 'hard', 'ly', 'said', 'to', 'move', '.', 'While', 'he', 'hesitated', 'how', 'to', 'proceed', ',', 'Le', 'Sub', 'til', '\\n', 'c', 'aut', 'iously', 'raised', 'himself', 'to', 'his', 'feet', ',', 'though', 'with', 'a', 'motion', 'so', 'slow', 'and', '\\n', 'gu', 'arded', ',', 'that', 'not', 'the', 'slight', 'est', 'noise', 'was', 'produced', 'by', 'the', 'change', '.', '\\n', 'Hey', 'ward', 'felt', 'it', 'had', 'now', 'become', 'inc', 'umb', 'ent', 'on', 'him', 'to', 'act', '.', 'Throw', 'ing', 'his', 'leg', '\\n', 'over', 'the', 'saddle', ',', 'he', 'dis', 'mount', 'ed', ',', 'with', 'a', 'determination', 'to', 'advance', 'and', '\\n', 'se', 'ize', 'his', 'tre', 'acher', 'ous', 'companion', ',', 'trust', 'ing', 'the', 'result', 'to', 'his', 'own', 'man', 'hood', '.', '\\n', 'In', 'order', ',', 'however', ',', 'to', 'prevent', 'unnecessary', 'alarm', ',', 'he', 'still', 'preserved', 'an', '\\n', 'air', 'of', 'calm', 'ness', 'and', 'friendship', '.', '\\n', '\\n', '\"', 'Le', 'Ren', 'ard', 'Sub', 'til', 'does', 'not', 'eat', ',\"', 'he', 'said', ',', 'using', 'the', 'app', 'ell', 'ation', 'he', 'had', '\\n', 'found', 'most', 'fl', 'attering', 'to', 'the', 'van', 'ity', 'of', 'the', 'Indian', '.', '\"', 'His', 'corn', 'is', 'not', 'well', '\\n', 'par', 'ched', ',', 'and', 'it', 'seems', 'dry', '.', 'Let', 'me', 'examine', ';', 'perhaps', 'something', 'may', 'be', '\\n', 'found', 'among', 'my', 'own', 'provisions', 'that', 'will', 'help', 'his', 'appetite', '.\"', '\\n', '\\n', 'Mag', 'ua', 'held', 'out', 'the', 'wallet', 'to', 'the', 'pro', 'ff', 'er', 'of', 'the', 'other', '.', 'He', 'even', 'suffered', '\\n', 'the', 'ir', 'hands', 'to', 'meet', ',', 'without', 'betray', 'ing', 'the', 'least', 'emotion', ',', 'or', 'varying', 'his', '\\n', 'riv', 'eted', 'attitude', 'of', 'attention', '.', 'But', 'when', 'he', 'felt', 'the', 'fingers', 'of', 'Hey', 'ward', '\\n', 'm', 'oving', 'gently', 'along', 'his', 'own', 'naked', 'arm', ',', 'he', 'struck', 'up', 'the', 'lim', 'b', 'of', 'the', '\\n', 'you', 'ng', 'man', ',', 'and', 'utter', 'ing', 'a', 'pier', 'cing', 'cry', 'as', 'he', 'd', 'art', 'ed', 'beneath', 'it', ',', 'pl', 'ung', 'ed', ',', '\\n', 'at', 'a', 'single', 'bound', ',', 'into', 'the', 'opposite', 'thick', 'et', '.', 'At', 'the', 'next', 'instant', 'the', '\\n', 'form', 'of', 'Ch', 'ing', 'ach', 'g', 'ook', 'appeared', 'from', 'the', 'bus', 'hes', ',', 'looking', 'like', 'a', 'spect', 're', 'in', '\\n', 'its', 'paint', ',', 'and', 'gl', 'ided', 'across', 'the', 'path', 'in', 'swift', 'pursuit', '.', 'Next', 'followed', '\\n', 'the', 'shout', 'of', 'Un', 'cas', ',', 'when', 'the', 'woods', 'were', 'light', 'ed', 'by', 'a', 'sudden', 'flash', ',', 'that', '\\n', 'was', 'accompanied', 'by', 'the', 'sharp', 'report', 'of', 'the', 'hun', 'ter', \"'\", 's', 'rifle', '.', '\\n', '\\n', '\\n', '\\n', '<s>', '{\"', 'name', '\":', '\"', 'Ch', 'apter', '', '4', '\",', '\"', 'url', '\":', '\"', 'https', '://', 'web', '.', 'archive', '.', 'org', '/', 'web', '/', '2', '0', '2', '0', '1', '1', '0', '1', '0', '5', '3', '2', '0', '5', '/', 'https', '://', 'www', '.', 'cl', 'iffs', 'notes', '.', 'com', '/', 'liter', 'ature', '/', 'l', '/', 'the', '-', 'last', '-', 'of', '-', 'the', '-', 'm', 'oh', 'icans', '/', 'summary', '-', 'and', '-', 'analysis', '/', 'ch', 'apter', '-', '4', '\",', '\"', 'summary', '\":', '\"', 'When', 'the', 'mounted', 'party', 'from', 'Fort', 'Howard', 'approaches', 'the', 'three', 'men', 'of', 'the', 'woods', ',', 'Haw', 'key', 'e', 'addresses', 'first', 'Gam', 'ut', 'and', 'then', 'Hey', 'ward', 'only', 'to', 'learn', 'that', 'they', 'are', 'lost', 'because', 'their', 'Indian', 'guide', 'has', 'taken', 'them', 'west', 'instead', 'of', 'north', 'toward', 'Fort', 'William', 'Henry', '.', 'Dou', 'bt', 'ful', ',', 'especially', 'when', 'he', 'learn', 's', 'that', 'the', 'guide', 'is', 'a', 'Hur', 'on', 'who', 'has', 'been', 'adopted', 'by', 'the', 'Moh', 'aw', 'ks', ',', 'Haw', 'key', 'e', 'makes', 'an', 'a', 'prior', 'i', 'judgment', 'of', 'the', 'still', '-', 'un', 'seen', 'guide', 'and', 'uses', 'the', 'cont', 'empt', 'uous', 'term', 'M', 'ingo', ':', '\\\\\"', 'he', 'who', 'is', 'born', 'a', 'M', 'ingo', 'will', 'die', 'a', 'M', 'ingo', '.', '\\\\\"', 'His', 'two', 'Indian', 'companions', 'con', 'cur', 'with', 'his', 'thinking', '.', 'Still', 'doub', 'ting', 'and', 'caut', 'ious', ',', 'be', 'b', 'ait', 's', 'Hey', 'ward', 'by', 'ban', 'ter', 'ing', 'away', 'about', 'Indians', 'until', 'Hey', 'ward', 'reveals', 'that', 'he', 'is', 'the', 'major', 'of', 'the', '', '6', '0', 'th', 'reg', 'iment', 'of', 'the', 'king', 'at', 'William', 'Henry', '.', 'Walk', 'ing', 'to', 'the', 'rear', 'of', 'the', 'party', 'for', 'a', 'look', 'at', 'Mag', 'ua', ',', 'Haw', 'key', 'e', 'returns', 'and', 'says', 'that', 'he', 'could', 'guide', 'them', 'back', 'to', 'Fort', 'Edward', ',', 'which', 'is', 'only', 'an', 'hour', \"'\", 's', 'journey', 'away', ',', 'but', 'that', 'it', 'would', 'be', 'impossible', 'because', 'of', 'the', 'ladies', 'and', 'the', 'd', 'angers', 'of', 'coming', 'night', ',', 'particularly', 'with', 'the', 'Moh', 'awk', 'as', 'a', 'companion', '.', 'He', 'suggests', 'his', 'shooting', 'and', 'dis', 'abling', 'Mag', 'ua', 'from', 'where', 'he', 'stands', ',', 'but', 'the', 'major', 'will', 'not', 'hear', 'of', 'it', '.', 'Con', 'sequently', ',', 'as', 'the', 'sun', 'goes', 'down', ',', 'he', 'sends', 'the', 'two', 'Moh', 'icans', 'through', 'the', 'thick', 'ets', 'on', 'opposite', 'sides', 'of', 'the', 'path', 'and', 'tells', 'the', 'major', 'to', 'engage', 'Mag', 'ua', 'in', 'talk', 'while', 'he', 'himself', 'convers', 'es', 'with', 'Gam', 'ut', '.', 'Mag', 'ua', 'proud', 'ly', 'refers', 'to', 'himself', 'as', 'Le', 'Ren', 'ard', 'Sub', 'til', ',', 'the', 'name', 'his', 'Canada', 'f', 'athers', 'have', 'given', 'him', '.', 'He', 'is', 'caut', 'iously', 'quiet', 'but', 'allows', 'Hey', 'ward', 'to', 'convince', 'him', 'to', 'sit', 'and', 'eat', '.', 'As', 'slight', 'sounds', 'in', 'the', 'thick', 'et', 'make', 'Le', 'Ren', 'ard', 'alert', ',', 'Hey', 'ward', 'dis', 'mount', 's', ',', 'determined', 'to', 'se', 'ize', 'the', 'tre', 'acher', 'ous', 'guide', ',', 'but', 'the', 'latter', 'strikes', 'up', 'the', 'major', \"'\", 's', 'arm', ',', 'gives', 'a', 'pier', 'cing', 'cry', ',', 'and', 'd', 'arts', 'away', 'into', 'the', 'thick', 'et', '.', 'Im', 'medi', 'ately', 'Ch', 'ing', 'ach', 'g', 'ook', 'and', 'Un', 'cas', 'appear', 'and', 'give', 'swift', 'pursuit', 'just', 'as', 'a', 'flash', 'comes', 'from', 'Haw', 'key', 'e', \"'\", 's', 'rifle', '.\",', '\"', 'analysis', '\":', '\"', 'Since', 'this', 'chapter', 'is', 'mostly', 'one', 'of', 'surface', 'action', ',', 'little', 'comment', 'is', 'needed', 'except', 'to', 'point', 'out', 'Haw', 'key', 'e', \"'\", 's', 'respect', 'for', 'the', 'military', 'and', 'the', 'fact', 'that', 'all', 'I', 'ro', 'qu', 'ois', 'tribes', 'are', 'to', 'be', 'looked', 'upon', 'as', 'tre', 'acher', 'ous', 'enemies', '.', 'The', 'alert', 'ness', 'and', 'swift', 'action', 'of', 'Mag', 'ua', ',', 'who', 'is', 'more', 'of', 'a', 'threat', 'when', 'they', 'do', 'not', 'know', 'his', 'where', 'ab', 'outs', ',', 'mark', 'him', 'as', 'a', 'worthy', 'opponent', 'for', 'the', 'st', 'al', 'wart', 'protagon', 'ists', '.', 'His', 'escape', 'height', 'ens', 'the', 'susp', 'ense', 'of', 'the', 'story', '.\"', '}']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(encoded_train_dataset['input_ids'][1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:42.500646500Z",
     "start_time": "2024-05-01T18:18:42.438942200Z"
    }
   },
   "id": "9b89e5448196482f"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 28705,\n 13,\n 28705,\n 345,\n 11273,\n 1167,\n 5080,\n 654,\n 480,\n 1334,\n 304,\n 261,\n 2636,\n 28725,\n 13,\n 2287,\n 11439,\n 298,\n 272,\n 1170,\n 321,\n 813,\n 24582,\n 4699,\n 286,\n 28745,\n 13,\n 28705,\n 415,\n 8970,\n 1174,\n 302,\n 15507,\n 6774,\n 13,\n 2287,\n 415,\n 6138,\n 304,\n 3585,\n 1503,\n 4768,\n 28745,\n 13,\n 28705,\n 1015,\n 5063,\n 1114,\n 28713,\n 26108,\n 28725,\n 304,\n 8191,\n 28718,\n 7835,\n 4226,\n 28725,\n 13,\n 2287,\n 1015,\n 285,\n 696,\n 1606,\n 668,\n 406,\n 286,\n 297,\n 272,\n 19759,\n 611,\n 13,\n 13,\n 28705,\n 19721,\n 28802,\n 12738,\n 28723,\n 13,\n 13,\n 13,\n 3514,\n 1652,\n 272,\n 10214,\n 381,\n 822,\n 288,\n 17162,\n 1050,\n 304,\n 516,\n 1885,\n 3269,\n 24804,\n 298,\n 13,\n 2748,\n 299,\n 6036,\n 1309,\n 13551,\n 778,\n 264,\n 8613,\n 369,\n 10932,\n 1259,\n 2655,\n 10333,\n 607,\n 13,\n 262,\n 15123,\n 28725,\n 478,\n 1580,\n 938,\n 396,\n 3227,\n 28742,\n 28713,\n 23037,\n 28725,\n 304,\n 6139,\n 272,\n 6337,\n 264,\n 1664,\n 13,\n 28719,\n 3429,\n 298,\n 272,\n 7635,\n 1050,\n 302,\n 272,\n 1633,\n 970,\n 478,\n 506,\n 1432,\n 2598,\n 706,\n 28723,\n 13,\n 13,\n 2486,\n 369,\n 1370,\n 28725,\n 989,\n 1683,\n 654,\n 17826,\n 2131,\n 356,\n 272,\n 13283,\n 302,\n 264,\n 1741,\n 562,\n 8421,\n 13,\n 3888,\n 28725,\n 2373,\n 396,\n 5115,\n 28742,\n 28713,\n 8123,\n 302,\n 272,\n 2524,\n 1057,\n 466,\n 302,\n 816,\n 1754,\n 28725,\n 737,\n 1395,\n 13,\n 12938,\n 4293,\n 286,\n 272,\n 9293,\n 302,\n 396,\n 20311,\n 1338,\n 28725,\n 442,\n 272,\n 4431,\n 302,\n 741,\n 13,\n 7398,\n 1951,\n 28723,\n 415,\n 9555,\n 541,\n 1600,\n 302,\n 16748,\n 6049,\n 3837,\n 298,\n 272,\n 9829,\n 302,\n 13,\n 1237,\n 7782,\n 754,\n 28716,\n 6185,\n 272,\n 2130,\n 28725,\n 304,\n 10504,\n 288,\n 871,\n 3199,\n 1868,\n 395,\n 264,\n 13,\n 450,\n 7928,\n 295,\n 441,\n 28723,\n 415,\n 408,\n 748,\n 302,\n 272,\n 4376,\n 654,\n 5398,\n 298,\n 2333,\n 2108,\n 27118,\n 28725,\n 304,\n 13,\n 1237,\n 14373,\n 6601,\n 302,\n 272,\n 1370,\n 403,\n 2108,\n 2106,\n 28725,\n 390,\n 272,\n 5106,\n 263,\n 363,\n 377,\n 734,\n 302,\n 272,\n 13,\n 7558,\n 28713,\n 304,\n 285,\n 696,\n 1606,\n 8536,\n 2747,\n 652,\n 15080,\n 28724,\n 21137,\n 28725,\n 304,\n 26916,\n 297,\n 272,\n 13,\n 270,\n 7186,\n 8800,\n 28723,\n 9054,\n 369,\n 14232,\n 9296,\n 28725,\n 690,\n 14191,\n 272,\n 281,\n 671,\n 3467,\n 13,\n 28713,\n 353,\n 434,\n 1494,\n 302,\n 396,\n 2556,\n 13894,\n 297,\n 4398,\n 28725,\n 660,\n 28728,\n 8744,\n 272,\n 427,\n 27596,\n 6297,\n 28725,\n 13,\n 1646,\n 17488,\n 865,\n 486,\n 272,\n 2859,\n 14549,\n 302,\n 272,\n 1683,\n 28725,\n 272,\n 20636,\n 304,\n 17898,\n 13,\n 28707,\n 377,\n 302,\n 264,\n 4768,\n 386,\n 13160,\n 28725,\n 272,\n 2312,\n 556,\n 440,\n 7843,\n 302,\n 741,\n 319,\n 5675,\n 28724,\n 461,\n 339,\n 28725,\n 442,\n 264,\n 1719,\n 3572,\n 13,\n 266,\n 272,\n 8120,\n 28725,\n 477,\n 272,\n 21287,\n 712,\n 283,\n 302,\n 264,\n 15569,\n 2130,\n 9197,\n 28723,\n 13,\n 13,\n 18171,\n 10351,\n 982,\n 304,\n 7098,\n 7258,\n 654,\n 28725,\n 3545,\n 28725,\n 1368,\n 8228,\n 298,\n 272,\n 13,\n 994,\n 12242,\n 28725,\n 298,\n 3924,\n 652,\n 4501,\n 477,\n 272,\n 680,\n 5853,\n 3209,\n 302,\n 13,\n 1237,\n 361,\n 19198,\n 28723,\n 4023,\n 624,\n 302,\n 1167,\n 1448,\n 1685,\n 404,\n 6642,\n 272,\n 2760,\n 4759,\n 304,\n 13,\n 28727,\n 666,\n 932,\n 406,\n 267,\n 1339,\n 302,\n 264,\n 8271,\n 302,\n 272,\n 16748,\n 28725,\n 272,\n 799,\n 8244,\n 1345,\n 28725,\n 13,\n 14968,\n 272,\n 5934,\n 302,\n 516,\n 26705,\n 304,\n 5597,\n 8639,\n 465,\n 4731,\n 1339,\n 28725,\n 272,\n 1170,\n 8918,\n 28725,\n 13,\n 3128,\n 4376,\n 7945,\n 448,\n 304,\n 1043,\n 28733,\n 28722,\n 8744,\n 4630,\n 296,\n 302,\n 624,\n 693,\n 1659,\n 3452,\n 24871,\n 13,\n 3211,\n 264,\n 6392,\n 2564,\n 465,\n 28723,\n 415,\n 4494,\n 403,\n 24732,\n 356,\n 272,\n 948,\n 302,\n 264,\n 290,\n 2158,\n 28724,\n 13,\n 1582,\n 28725,\n 297,\n 264,\n 1704,\n 482,\n 369,\n 15463,\n 713,\n 298,\n 5110,\n 269,\n 272,\n 2030,\n 302,\n 516,\n 13,\n 644,\n 28711,\n 374,\n 3842,\n 28725,\n 486,\n 272,\n 10325,\n 562,\n 4072,\n 495,\n 7165,\n 1238,\n 302,\n 396,\n 6735,\n 13,\n 980,\n 2569,\n 297,\n 12745,\n 28723,\n 2354,\n 2187,\n 28725,\n 690,\n 403,\n 5597,\n 15913,\n 28725,\n 7567,\n 264,\n 13,\n 360,\n 16878,\n 877,\n 11706,\n 302,\n 3168,\n 28725,\n 10421,\n 297,\n 791,\n 4082,\n 1006,\n 9304,\n 302,\n 3075,\n 304,\n 13,\n 10323,\n 28723,\n 2354,\n 11640,\n 480,\n 4393,\n 1335,\n 28725,\n 356,\n 690,\n 708,\n 799,\n 3691,\n 821,\n 272,\n 1162,\n 13,\n 4717,\n 304,\n 484,\n 3098,\n 20162,\n 10431,\n 2917,\n 8582,\n 632,\n 28792,\n 28782,\n 28793,\n 403,\n 23910,\n 28725,\n 403,\n 1671,\n 13,\n 1334,\n 1686,\n 302,\n 707,\n 2112,\n 28725,\n 395,\n 272,\n 5851,\n 302,\n 264,\n 2128,\n 11969,\n 317,\n 17968,\n 28742,\n 28713,\n 549,\n 2150,\n 28725,\n 13,\n 6087,\n 12554,\n 516,\n 22718,\n 28725,\n 304,\n 3289,\n 286,\n 754,\n 272,\n 1749,\n 7793,\n 28723,\n 330,\n 298,\n 705,\n 28716,\n 22527,\n 13,\n 391,\n 10431,\n 2917,\n 28733,\n 13223,\n 1027,\n 28725,\n 302,\n 4300,\n 6298,\n 482,\n 28725,\n 654,\n 297,\n 516,\n 319,\n 1844,\n 291,\n 28745,\n 1312,\n 264,\n 13,\n 10046,\n 5469,\n 19427,\n 28725,\n 302,\n 369,\n 3127,\n 395,\n 690,\n 272,\n 4920,\n 302,\n 272,\n 19719,\n 13,\n 22508,\n 652,\n 8639,\n 465,\n 23180,\n 28725,\n 4897,\n 1656,\n 12298,\n 2673,\n 516,\n 13034,\n 304,\n 268,\n 473,\n 14589,\n 13,\n 28729,\n 485,\n 28706,\n 28723,\n 415,\n 14708,\n 8118,\n 28725,\n 2173,\n 8723,\n 2132,\n 1816,\n 28725,\n 304,\n 14272,\n 2113,\n 269,\n 617,\n 302,\n 13,\n 894,\n 25838,\n 28725,\n 682,\n 14543,\n 369,\n 400,\n 553,\n 5048,\n 272,\n 15068,\n 271,\n 302,\n 516,\n 2202,\n 28725,\n 13,\n 3128,\n 708,\n 12380,\n 302,\n 15224,\n 6178,\n 298,\n 506,\n 2783,\n 6334,\n 2106,\n 516,\n 676,\n 5079,\n 28723,\n 13,\n 13,\n 1014,\n 4108,\n 302,\n 272,\n 3075,\n 676,\n 28725,\n 4612,\n 3080,\n 486,\n 1259,\n 5099,\n 390,\n 654,\n 459,\n 13270,\n 4742,\n 13,\n 1403,\n 516,\n 8155,\n 28725,\n 403,\n 737,\n 369,\n 302,\n 624,\n 693,\n 553,\n 2651,\n 1856,\n 14498,\n 304,\n 13,\n 720,\n 930,\n 296,\n 477,\n 516,\n 21864,\n 9539,\n 28723,\n 2354,\n 1338,\n 28725,\n 2070,\n 2009,\n 12723,\n 28725,\n 403,\n 13,\n 28712,\n 1223,\n 998,\n 4265,\n 601,\n 821,\n 2173,\n 28745,\n 562,\n 1012,\n 23284,\n 304,\n 14540,\n 6178,\n 1117,\n 969,\n 13,\n 391,\n 1176,\n 324,\n 601,\n 486,\n 521,\n 1569,\n 3309,\n 15368,\n 304,\n 298,\n 309,\n 28723,\n 650,\n 12003,\n 264,\n 16268,\n 28733,\n 21122,\n 13,\n 1009,\n 8613,\n 5344,\n 28725,\n 285,\n 699,\n 286,\n 395,\n 23716,\n 9684,\n 28792,\n 28784,\n 1181,\n 304,\n 264,\n 5561,\n 2058,\n 302,\n 1321,\n 1126,\n 13,\n 6974,\n 553,\n 750,\n 480,\n 1334,\n 302,\n 652,\n 2982,\n 28723,\n 650,\n 835,\n 22809,\n 264,\n 12579,\n 297,\n 264,\n 319,\n 1844,\n 291,\n 302,\n 13,\n 28727,\n 1057,\n 383,\n 28725,\n 737,\n 369,\n 690,\n 1885,\n 1311,\n 272,\n 752,\n 440,\n 28724,\n 5925,\n 1339,\n 302,\n 272,\n 6735,\n 28725,\n 562,\n 13,\n 1510,\n 298,\n 705,\n 28716,\n 22527,\n 28723,\n 2354,\n 290,\n 402,\n 11768,\n 1126,\n 654,\n 442,\n 10059,\n 286,\n 1024,\n 272,\n 11997,\n 8844,\n 302,\n 272,\n 13,\n 28711,\n 5087,\n 28725,\n 1312,\n 272,\n 865,\n 744,\n 302,\n 516,\n 916,\n 28733,\n 28715,\n 638,\n 690,\n 6178,\n 3624,\n 272,\n 13,\n 28716,\n 20128,\n 28733,\n 28722,\n 16013,\n 28725,\n 403,\n 264,\n 5964,\n 302,\n 9918,\n 24208,\n 462,\n 1576,\n 742,\n 28725,\n 369,\n 305,\n 2701,\n 438,\n 272,\n 8480,\n 28725,\n 13,\n 391,\n 690,\n 654,\n 319,\n 4136,\n 286,\n 2747,\n 272,\n 13431,\n 395,\n 272,\n 268,\n 473,\n 6420,\n 302,\n 264,\n 27002,\n 28723,\n 330,\n 13,\n 28720,\n 3204,\n 304,\n 16624,\n 7368,\n 516,\n ...]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset['input_ids'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:46.000992600Z",
     "start_time": "2024-05-01T18:18:45.966142800Z"
    }
   },
   "id": "dd0cfef91d4969f4"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_train_dataset['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:47.241821Z",
     "start_time": "2024-05-01T18:18:47.194161100Z"
    }
   },
   "id": "45372df9a4c8429e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chapter', 'summary', 'input_ids', 'attention_mask'],\n    num_rows: 10\n})"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_val_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:48.366843100Z",
     "start_time": "2024-05-01T18:18:48.360970900Z"
    }
   },
   "id": "bcc5687c71876ce3"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chapter', 'summary', 'input_ids', 'attention_mask'],\n    num_rows: 10\n})"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:49.171746800Z",
     "start_time": "2024-05-01T18:18:49.167605800Z"
    }
   },
   "id": "da0f652ddcdf131d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# encoded_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:50.438270400Z",
     "start_time": "2024-05-01T18:18:50.426760800Z"
    }
   },
   "id": "606630419739dc2b"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=0.001,\n",
    "                                  optim='adamw_torch'\n",
    "                                  )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:53.692950400Z",
     "start_time": "2024-05-01T18:18:53.640849100Z"
    }
   },
   "id": "6409409bf6ac81a3"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:55.041970400Z",
     "start_time": "2024-05-01T18:18:54.408663900Z"
    }
   },
   "id": "4b2da725494d317a"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Adapter with name default already exists. Please use a different name.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\integrations\\peft.py:254\u001B[0m, in \u001B[0;36mPeftAdapterMixin.add_adapter\u001B[1;34m(self, adapter_config, adapter_name)\u001B[0m\n\u001B[0;32m    252\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hf_peft_config_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m adapter_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config:\n\u001B[1;32m--> 254\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdapter with name \u001B[39m\u001B[38;5;132;01m{\u001B[39;00madapter_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m already exists. Please use a different name.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(adapter_config, PeftConfig):\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madapter_config should be an instance of PeftConfig. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(adapter_config)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    259\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Adapter with name default already exists. Please use a different name."
     ]
    }
   ],
   "source": [
    "model.add_adapter(peft_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:18:55.917997700Z",
     "start_time": "2024-05-01T18:18:55.039967600Z"
    }
   },
   "id": "705596b4ab2ec935"
  },
  {
   "cell_type": "markdown",
   "source": [
    "How to Use Rouge\n",
    "At minimum, this metric takes as input a list of predictions and a list of references:\n",
    "\n",
    ">>> rouge = evaluate.load('rouge')\n",
    ">>> predictions = [\"hello there\", \"general kenobi\"]\n",
    ">>> references = [\"hello there\", \"general kenobi\"]\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references)\n",
    ">>> print(results)\n",
    "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
    "One can also pass a custom tokenizer which is especially useful for non-latin languages.\n",
    "\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references,\n",
    "                            tokenizer=lambda x: x.split())\n",
    ">>> print(results)\n",
    "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
    "It can also deal with lists of references for each predictions:\n",
    "\n",
    ">>> rouge = evaluate.load('rouge')\n",
    ">>> predictions = [\"hello there\", \"general kenobi\"]\n",
    ">>> references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references)\n",
    ">>> print(results)\n",
    "{'rouge1': 0.8333, 'rouge2': 0.5, 'rougeL': 0.8333, 'rougeLsum': 0.8333}```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ef91ae2fe690d73"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_val_dataset,\n",
    "    compute_metrics=rouge\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:19:00.110302Z",
     "start_time": "2024-05-01T18:19:00.099673200Z"
    }
   },
   "id": "7586a4126c2f95c7"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.86 GiB is allocated by PyTorch, and 358.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\trainer.py:1780\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1778\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1780\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1781\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1782\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1783\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1784\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1785\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\trainer.py:2118\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2117\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2118\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2121\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2122\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2123\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2124\u001B[0m ):\n\u001B[0;32m   2125\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2126\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\trainer.py:3036\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   3033\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   3035\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[1;32m-> 3036\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3038\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3039\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\trainer.py:3059\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[0;32m   3057\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3058\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 3059\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3060\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[0;32m   3061\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[0;32m   3062\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:1157\u001B[0m, in \u001B[0;36mMistralForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1154\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   1156\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1163\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1165\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1167\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1169\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1170\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:1042\u001B[0m, in \u001B[0;36mMistralModel.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1032\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m   1033\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m   1034\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1039\u001B[0m         use_cache,\n\u001B[0;32m   1040\u001B[0m     )\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1042\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1043\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1044\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1045\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1046\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1047\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1048\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1049\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1051\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1053\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:770\u001B[0m, in \u001B[0;36mMistralDecoderLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001B[0m\n\u001B[0;32m    768\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    769\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[1;32m--> 770\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    771\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[0;32m    773\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:179\u001B[0m, in \u001B[0;36mMistralMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_fn(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mup_proj(x))\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:452\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[1;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[0;32m    450\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_layer(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 452\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001B[39;00m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001B[39;00m\n\u001B[0;32m    455\u001B[0m     \u001B[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001B[39;00m\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001B[39;00m\n\u001B[0;32m    457\u001B[0m     \u001B[38;5;66;03m# sure.\u001B[39;00m\n\u001B[0;32m    458\u001B[0m     result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mclone()\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:468\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    465\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n\u001B[0;32m    467\u001B[0m bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n\u001B[1;32m--> 468\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mbnb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul_4bit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    470\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mto(inp_dtype)\n\u001B[0;32m    472\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:579\u001B[0m, in \u001B[0;36mmatmul_4bit\u001B[1;34m(A, B, quant_state, out, bias)\u001B[0m\n\u001B[0;32m    577\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 579\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMatMul4Bit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\torch\\autograd\\function.py:553\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    552\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    557\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    558\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    559\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    560\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    561\u001B[0m     )\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:509\u001B[0m, in \u001B[0;36mMatMul4Bit.forward\u001B[1;34m(ctx, A, B, out, bias, quant_state)\u001B[0m\n\u001B[0;32m    505\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mempty(A\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m B_shape[:\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    507\u001B[0m \u001B[38;5;66;03m# 1. Dequantize\u001B[39;00m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;66;03m# 2. MatmulnN\u001B[39;00m\n\u001B[1;32m--> 509\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdequantize_4bit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;66;03m# 3. Save state\u001B[39;00m\n\u001B[0;32m    512\u001B[0m ctx\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m quant_state\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.86 GiB is allocated by PyTorch, and 358.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T18:20:00.913359700Z",
     "start_time": "2024-05-01T18:19:01.189860700Z"
    }
   },
   "id": "6366767320253bfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T20:22:27.503202300Z",
     "start_time": "2024-04-28T20:22:27.503202300Z"
    }
   },
   "id": "4bf017f90b6cb5eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(encoded_train_dataset['attention_mask'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T20:22:27.504421800Z",
     "start_time": "2024-04-28T20:22:27.504421800Z"
    }
   },
   "id": "943bbffbc30f0e5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(encoded_train_dataset['attention_mask'][78]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T20:22:27.505428900Z"
    }
   },
   "id": "29712cac9bb0dcba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "80221280ffa61a47"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[72], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(encoded_train_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mencoded_train_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[i]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m11593\u001B[39m:\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28mprint\u001B[39m(i)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2810\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2808\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[0;32m   2809\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2810\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2795\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[1;34m(self, key, **kwargs)\u001B[0m\n\u001B[0;32m   2793\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[0;32m   2794\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m query_table(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data, key, indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices)\n\u001B[1;32m-> 2795\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m \u001B[43mformat_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpa_subtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformat_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_all_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_all_columns\u001B[49m\n\u001B[0;32m   2797\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2798\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001B[0m, in \u001B[0;36mformat_table\u001B[1;34m(table, key, formatter, format_columns, output_all_columns)\u001B[0m\n\u001B[0;32m    627\u001B[0m python_formatter \u001B[38;5;241m=\u001B[39m PythonFormatter(features\u001B[38;5;241m=\u001B[39mformatter\u001B[38;5;241m.\u001B[39mfeatures)\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m format_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m format_columns:\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:398\u001B[0m, in \u001B[0;36mFormatter.__call__\u001B[1;34m(self, pa_table, query_type)\u001B[0m\n\u001B[0;32m    396\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat_row(pa_table)\n\u001B[0;32m    397\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat_batch(pa_table)\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:442\u001B[0m, in \u001B[0;36mPythonFormatter.format_column\u001B[1;34m(self, pa_table)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    441\u001B[0m     column \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_arrow_extractor()\u001B[38;5;241m.\u001B[39mextract_column(pa_table)\n\u001B[1;32m--> 442\u001B[0m     column \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpython_features_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpa_table\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    443\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m column\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:217\u001B[0m, in \u001B[0;36mPythonFeaturesDecoder.decode_column\u001B[1;34m(self, column, column_name)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_row\u001B[39m(\u001B[38;5;28mself\u001B[39m, row: \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mdecode_example(row) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;28;01melse\u001B[39;00m row\n\u001B[1;32m--> 217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, column: \u001B[38;5;28mlist\u001B[39m, column_name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mdecode_column(column, column_name) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;28;01melse\u001B[39;00m column\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_batch\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(encoded_train_dataset['input_ids'])):\n",
    "    if len(encoded_train_dataset['input_ids'][i]) == 11593:\n",
    "        print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T18:25:18.479775400Z",
     "start_time": "2024-04-28T18:18:42.617101200Z"
    }
   },
   "id": "83dd58e6b4acf72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b2616ae4e0788344"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
