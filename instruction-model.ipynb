{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:31:50.702490Z",
     "start_time": "2024-05-05T09:31:46.969900Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:31:52.513491Z",
     "start_time": "2024-05-05T09:31:52.509364Z"
    }
   },
   "id": "c7115aa70dce7561",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_rslora=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:31:54.000264Z",
     "start_time": "2024-05-05T09:31:53.997293Z"
    }
   },
   "id": "a807a9179fb53681",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"pszemraj/booksum-short\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:02.343263Z",
     "start_time": "2024-05-05T09:31:56.086687Z"
    }
   },
   "id": "7263fc51f1ae225f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:04.177484Z",
     "start_time": "2024-05-05T09:32:04.170187Z"
    }
   },
   "id": "31b9d8feb4fa7404",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
       "        num_rows: 5912\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
       "        num_rows: 1012\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
       "        num_rows: 988\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        # load_in_8bit=True,\n",
    "        load_in_4bit=True,\n",
    "        # llm_int8_enable_fp32_cpu_offload=True,\n",
    "        # llm_int8_has_fp16_weight=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:05.047287Z",
     "start_time": "2024-05-05T09:32:05.041147Z"
    }
   },
   "id": "261dd87971875d73",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:06.320118Z",
     "start_time": "2024-05-05T09:32:06.317028Z"
    }
   },
   "id": "89ea41671bac7df1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:20.534375Z",
     "start_time": "2024-05-05T09:32:07.022887Z"
    }
   },
   "id": "1a28c0ec89c553f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e570ab21c6f14a68ac31946411c17a8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:29.395127Z",
     "start_time": "2024-05-05T09:32:29.391968Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.rope_scaling = {\"type\": \"linear\", \"factor\": 0.3}",
   "id": "8a26b9aeed193454",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:30.519802Z",
     "start_time": "2024-05-05T09:32:30.352250Z"
    }
   },
   "id": "23934443aa3ee2a3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:31.190972Z",
     "start_time": "2024-05-05T09:32:31.187634Z"
    }
   },
   "id": "992cca422d0efe21",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=10000)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:48.987582Z",
     "start_time": "2024-05-05T09:32:32.255342Z"
    }
   },
   "id": "b4daafb425acfd93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "C:\\Users\\yferc\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Yes, I can definitely provide you with a classic Mayonnaise recipe. Here's a simple one:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup vegetable oil (canola, corn or safflower are good choices)\n",
      "- 1 tablespoon white wine vinegar or distilled white vinegar\n",
      "- 1 tablespoon fresh lemon juice\n",
      "- 1 teaspoon Dijon mustard\n",
      "- 1 teaspoon salt\n",
      "- 1 1/2 teaspoons minced garlic (optional)\n",
      "- 1 large egg yolk (at room temperature)\n",
      "\n",
      "Instructions:\n",
      "1. In a medium bowl, whisk together vinegar, lemon juice, mustard, salt, and garlic (if using) until well combined.\n",
      "2. Let the bowl rest at room temperature for 5 minutes, this is called the \"resting period\". This will allow the ingredients to hydrate the egg yolk and make the emulsion easier to form.\n",
      "3. Slowly, drop by drop, while continually whisking, add 1/2 cup of the oil into the yolk mixture until it thickens and emulsifies. You'll know it's working when the mixture thickens and the oil forms ribbons instead of just blending in.\n",
      "4. Once the first 1/2 cup of oil is emulsified, increase the speed of your whisking and begin to add the remaining 1/2 cup of oil very gradually, in a thin, steady stream. Make sure that the oil is absorbed by the mixture before you add more.\n",
      "5. Once all the oil is incorporated, taste the mayonnaise and adjust seasoning as needed (e.g. add more salt, vinegar, or lemon juice). Refrigerate it right away.\n",
      "6. Store homemade mayonnaise in a tightly sealed container in the refrigerator for up to one week.\n",
      "\n",
      "Remember, raw eggs carry a risk of Salmonella, so consume your homemade mayonnaise within a week and pay close attention to its freshness. Enjoy!</s>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = dataset[\"train\"].select_columns(['chapter', 'summary']).select(range(10))\n",
    "val_dataset = dataset[\"validation\"].select_columns(['chapter', 'summary']).select(range(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:48.998208Z",
     "start_time": "2024-05-05T09:32:48.988587Z"
    }
   },
   "id": "b17151a120d20d86",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:54.770401Z",
     "start_time": "2024-05-05T09:32:54.767203Z"
    }
   },
   "id": "82ddcf8f14a29e68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chapter', 'summary'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:55.519549Z",
     "start_time": "2024-05-05T09:32:55.516430Z"
    }
   },
   "id": "67ac210affa99d5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
      "        num_rows: 5912\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
      "        num_rows: 1012\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length'],\n",
      "        num_rows: 988\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following chapter of a book.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + chapter + end_prompt for chapter in example[\"chapter\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=1000).input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding='max_length',truncation=True, return_tensors=\"pt\", max_length=1000).input_ids\n",
    "\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:56.260033Z",
     "start_time": "2024-05-05T09:32:56.256351Z"
    }
   },
   "id": "5b53a32fd03f2b1b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# def encode(dataset):\n",
    "#     return tokenizer(dataset[\"chapter\"], dataset[\"summary\"], truncation=True, padding=\"max_length\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:57.071254Z",
     "start_time": "2024-05-05T09:32:57.068279Z"
    }
   },
   "id": "a21c8d680b03aa8f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "encoded_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# encoded_train_dataset = train_dataset.map(encode)\n",
    "# encoded_val_dataset = val_dataset.map(encode)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:57.796496Z",
     "start_time": "2024-05-05T09:32:57.747803Z"
    }
   },
   "id": "21b6dcf401ac90a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_train_dataset = encoded_train_dataset.remove_columns(['chapter', 'summary'])\n",
    "encoded_val_dataset = encoded_val_dataset.remove_columns(['chapter', 'summary'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:58.554030Z",
     "start_time": "2024-05-05T09:32:58.547694Z"
    }
   },
   "id": "a0bd59bf5c4c08d0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:32:59.253976Z",
     "start_time": "2024-05-05T09:32:59.250485Z"
    }
   },
   "id": "570b7e49609a049",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "len(encoded_train_dataset['input_ids'][3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:00.138589Z",
     "start_time": "2024-05-05T09:33:00.127694Z"
    }
   },
   "id": "a567360114da9c78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:00.807042Z",
     "start_time": "2024-05-05T09:33:00.800042Z"
    }
   },
   "cell_type": "code",
   "source": "len(encoded_train_dataset['input_ids'][0])",
   "id": "c241720197480194",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenizer.batch_decode(encoded_train_dataset['input_ids'][1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:01.602821Z",
     "start_time": "2024-05-05T09:33:01.590086Z"
    }
   },
   "id": "9b89e5448196482f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Sum', 'mar', 'ize', 'the', 'following', 'chapter', 'of', 'a', 'book', '.', '\\n', '\\n', '\\n', '', '\"', 'Well', ',', 'go', 'thy', 'way', ':', 'thou', 'sh', 'alt', 'not', 'from', 'this', 'gro', 've', '\\n', '', 'T', 'ill', 'I', 'tor', 'ment', 'the', 'e', 'for', 'this', 'injury', '.\"', '\\n', '\\n', '', '_', 'M', 'id', 'sum', 'mer', 'Night', \"'\", 's', 'Dream', '._', '\\n', '\\n', '\\n', 'The', 'words', 'were', 'still', 'in', 'the', 'mouth', 'of', 'the', 'sc', 'out', ',', 'when', 'the', 'leader', 'of', 'the', '\\n', 'party', ',', 'whose', 'approaching', 'foot', 'steps', 'had', 'caught', 'the', 'vig', 'il', 'ant', 'ear', 'of', 'the', '\\n', 'Ind', 'ian', ',', 'came', 'openly', 'into', 'view', '.', 'A', 'beaten', 'path', ',', 'such', 'as', 'those', 'made', 'by', 'the', '\\n', 'period', 'ical', 'passage', 'of', 'the', 'deer', ',', 'wound', 'through', 'a', 'little', 'gl', 'en', 'at', 'no', 'great', '\\n', 'distance', ',', 'and', 'struck', 'the', 'river', 'at', 'the', 'point', 'where', 'the', 'white', 'man', 'and', 'his', '\\n', 'red', 'companions', 'had', 'posted', 'themselves', '.', 'Along', 'this', 'track', 'the', 'trav', 'ellers', ',', '\\n', 'who', 'had', 'produced', 'a', 'surprise', 'so', 'unusual', 'in', 'the', 'depth', 's', 'of', 'the', 'forest', ',', '\\n', 'adv', 'anced', 'slowly', 'towards', 'the', 'hun', 'ter', ',', 'who', 'was', 'in', 'front', 'of', 'his', 'associ', 'ates', ',', '\\n', 'in', 'read', 'iness', 'to', 'receive', 'them', '.', '\\n', '\\n', '\"', 'Who', 'comes', '?\"', 'demanded', 'the', 'sc', 'out', ',', 'throwing', 'his', 'rifle', 'care', 'lessly', 'across', '\\n', 'his', 'left', 'arm', ',', 'and', 'keeping', 'the', 'fore', 'f', 'inger', 'of', 'his', 'right', 'hand', 'on', 'the', '\\n', 'trigger', ',', 'though', 'he', 'avoided', 'all', 'appearance', 'of', 'men', 'ace', 'in', 'the', 'act', ',', '\"', 'Who', '\\n', 'comes', 'h', 'ither', ',', 'among', 'the', 'be', 'asts', 'and', 'd', 'angers', 'of', 'the', 'wild', 'erness', '?\"', '\\n', '\\n', '\"', 'Bel', 'ie', 'vers', 'in', 'religion', ',', 'and', 'friends', 'to', 'the', 'law', 'and', 'to', 'the', 'king', ',\"', '\\n', 'return', 'ed', 'he', 'who', 'rode', 'fore', 'most', '.', '\"', 'Men', 'who', 'have', 'journey', 'ed', 'since', 'the', 'rising', '\\n', 'sun', ',', 'in', 'the', 'sh', 'ades', 'of', 'this', 'forest', ',', 'without', 'n', 'our', 'ishment', ',', 'and', 'are', 'sadly', '\\n', 't', 'ired', 'of', 'their', 'way', 'f', 'aring', '.\"', '\\n', '\\n', '\"', 'You', 'are', ',', 'then', ',', 'lost', ',\"', 'interrupted', 'the', 'hun', 'ter', ',', '\"', 'and', 'have', 'found', 'how', '\\n', 'hel', 'pless', \"'\", 't', 'is', 'not', 'to', 'know', 'whether', 'to', 'take', 'the', 'right', 'hand', 'or', 'the', 'left', '?\"', '\\n', '\\n', '\"', 'Even', 'so', ';', 'suck', 'ing', 'bab', 'es', 'are', 'not', 'more', 'dependent', 'on', 'those', 'who', 'guide', 'them', '\\n', 'than', 'we', 'who', 'are', 'of', 'larger', 'growth', ',', 'and', 'who', 'may', 'now', 'be', 'said', 'to', 'possess', 'the', '\\n', 'st', 'ature', 'without', 'the', 'knowledge', 'of', 'men', '.', 'Know', 'you', 'the', 'distance', 'to', 'a', 'post', 'of', '\\n', 'the', 'crown', 'called', 'William', 'Henry', '?\"', '\\n', '\\n', '\"', 'H', 'oot', '!\"', 'shouted', 'the', 'sc', 'out', ',', 'who', 'did', 'not', 'spare', 'his', 'open', 'laughter', ',', 'though', ',', '\\n', 'inst', 'antly', 'checking', 'the', 'dangerous', 'sounds', ',', 'he', 'indul', 'ged', 'his', 'm', 'err', 'iment', 'at', '\\n', 'less', 'risk', 'of', 'being', 'over', 'he', 'ard', 'by', 'any', 'lur', 'king', 'enemies', '.', '\"', 'You', 'are', 'as', 'much', '\\n', 'off', 'the', 'scent', 'as', 'a', 'h', 'ound', 'would', 'be', ',', 'with', 'Hor', 'ican', 'at', 'wi', 'xt', 'him', 'and', 'the', 'deer', '!', '\\n', 'Will', 'iam', 'Henry', ',', 'man', '!', 'if', 'you', 'are', 'friends', 'to', 'the', 'king', ',', 'and', 'have', 'business', '\\n', 'with', 'the', 'army', ',', 'your', 'better', 'way', 'would', 'be', 'to', 'follow', 'the', 'river', 'down', 'to', '\\n', 'Ed', 'ward', ',', 'and', 'lay', 'the', 'matter', 'before', 'We', 'bb', ';', 'who', 'tar', 'ries', 'there', ',', 'instead', 'of', '\\n', 'push', 'ing', 'into', 'the', 'def', 'iles', ',', 'and', 'driving', 'this', 'sau', 'cy', 'French', 'man', 'back', 'across', '\\n', 'Ch', 'am', 'plain', ',', 'into', 'his', 'den', 'again', '.\"', '\\n', '\\n', 'Before', 'the', 'stranger', 'could', 'make', 'any', 'reply', 'to', 'this', 'unexpected', 'proposition', ',', '\\n', 'an', 'other', 'horse', 'man', 'dashed', 'the', 'bus', 'hes', 'aside', ',', 'and', 'le', 'aped', 'his', 'charg', 'er', 'into', '\\n', 'the', 'path', 'way', ',', 'in', 'front', 'of', 'his', 'companion', '.', '\\n', '\\n', '\"', 'What', ',', 'then', ',', 'may', 'be', 'our', 'distance', 'from', 'Fort', 'Edward', '?\"', 'demanded', 'a', 'new', '\\n', 'spe', 'aker', ';', '\"', 'the', 'place', 'you', 'advise', 'us', 'to', 'seek', 'we', 'left', 'this', 'morning', ',', 'and', 'our', '\\n', 'destination', 'is', 'the', 'head', 'of', 'the', 'lake', '.\"', '\\n', '\\n', '\"', 'Then', 'you', 'must', 'have', 'lost', 'your', 'eyes', 'ight', 'a', 'fore', 'losing', 'your', 'way', ',', 'for', 'the', '\\n', 'road', 'across', 'the', 'port', 'age', 'is', 'cut', 'to', 'a', 'good', 'two', 'ro', 'ds', ',', 'and', 'is', 'as', 'grand', 'a', '\\n', 'path', ',', 'I', 'calculate', ',', 'as', 'any', 'that', 'runs', 'into', 'London', ',', 'or', 'even', 'before', 'the', '\\n', 'pal', 'ace', 'of', 'the', 'king', 'himself', '.\"', '\\n', '\\n', '\"', 'We', 'will', 'not', 'dispute', 'concerning', 'the', 'excell', 'ence', 'of', 'the', 'passage', ',\"', 'returned', '\\n', 'Hey', 'ward', ',', 'smiling', ';', 'for', ',', 'as', 'the', 'reader', 'has', 'anticipated', ',', 'it', 'was', 'he', '.', '\"', 'It', 'is', '\\n', 'en', 'ough', ',', 'for', 'the', 'present', ',', 'that', 'we', 'trusted', 'to', 'an', 'Indian', 'guide', 'to', 'take', 'us', '\\n', 'by', 'a', 'near', 'er', ',', 'though', 'bl', 'inder', 'path', ',', 'and', 'that', 'we', 'are', 'de', 'ceived', 'in', 'his', '\\n', 'know', 'ledge', '.', 'In', 'plain', 'words', ',', 'we', 'know', 'not', 'where', 'we', 'are', '.\"', '\\n', '\\n', '\"', 'An', 'Indian', 'lost', 'in', 'the', 'woods', '!\"', 'said', 'the', 'sc', 'out', ',', 'shaking', 'his', 'head', '\\n', 'd', 'ou', 'b', 'ting', 'ly', ';', '\"', 'when', 'the', 'sun', 'is', 'sc', 'or', 'ching', 'the', 'tree', '-', 't', 'ops', ',', 'and', 'the', '\\n', 'water', '-', 'c', 'ourses', 'are', 'full', ';', 'when', 'the', 'm', 'oss', 'on', 'every', 'be', 'ech', 'he', 'sees', ',', 'will', 'tell', '\\n', 'him', 'in', 'which', 'quarter', 'the', 'north', 'star', 'will', 'shine', 'at', 'night', '!', 'The', 'woods', 'are', '\\n', 'full', 'of', 'deer', 'paths', 'which', 'run', 'to', 'the', 'streams', 'and', 'l', 'icks', ',', 'places', 'well', 'known', '\\n', 'to', 'everybody', ';', 'nor', 'have', 'the', 'ge', 'ese', 'done', 'their', 'flight', 'to', 'the', 'Canada', 'waters', '\\n', 'alt', 'ogether', '!', \"'\", 'T', 'is', 'strange', 'that', 'an', 'Indian', 'should', 'be', 'lost', 'at', 'wi', 'xt', 'Hor', 'ican', '\\n', 'and', 'the', 'bend', 'in', 'the', 'river', '.', 'Is', 'he', 'a', 'Moh', 'awk', '?\"', '\\n', '\\n', '\"', 'Not', 'by', 'birth', ',', 'though', 'adopted', 'in', 'that', 'tribe', ';', 'I', 'think', 'his', 'birth', 'place', 'was', '\\n', 'far', 'ther', 'north']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_train_dataset['input_ids'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:02.403942Z",
     "start_time": "2024-05-05T09:33:02.390390Z"
    }
   },
   "id": "dd0cfef91d4969f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 6927,\n",
       " 3479,\n",
       " 653,\n",
       " 272,\n",
       " 2296,\n",
       " 10661,\n",
       " 302,\n",
       " 264,\n",
       " 1820,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 28705,\n",
       " 345,\n",
       " 11273,\n",
       " 1167,\n",
       " 5080,\n",
       " 654,\n",
       " 480,\n",
       " 1334,\n",
       " 304,\n",
       " 261,\n",
       " 2636,\n",
       " 28725,\n",
       " 13,\n",
       " 2287,\n",
       " 11439,\n",
       " 298,\n",
       " 272,\n",
       " 1170,\n",
       " 321,\n",
       " 813,\n",
       " 24582,\n",
       " 4699,\n",
       " 286,\n",
       " 28745,\n",
       " 13,\n",
       " 28705,\n",
       " 415,\n",
       " 8970,\n",
       " 1174,\n",
       " 302,\n",
       " 15507,\n",
       " 6774,\n",
       " 13,\n",
       " 2287,\n",
       " 415,\n",
       " 6138,\n",
       " 304,\n",
       " 3585,\n",
       " 1503,\n",
       " 4768,\n",
       " 28745,\n",
       " 13,\n",
       " 28705,\n",
       " 1015,\n",
       " 5063,\n",
       " 1114,\n",
       " 28713,\n",
       " 26108,\n",
       " 28725,\n",
       " 304,\n",
       " 8191,\n",
       " 28718,\n",
       " 7835,\n",
       " 4226,\n",
       " 28725,\n",
       " 13,\n",
       " 2287,\n",
       " 1015,\n",
       " 285,\n",
       " 696,\n",
       " 1606,\n",
       " 668,\n",
       " 406,\n",
       " 286,\n",
       " 297,\n",
       " 272,\n",
       " 19759,\n",
       " 611,\n",
       " 13,\n",
       " 13,\n",
       " 28705,\n",
       " 19721,\n",
       " 28802,\n",
       " 12738,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 3514,\n",
       " 1652,\n",
       " 272,\n",
       " 10214,\n",
       " 381,\n",
       " 822,\n",
       " 288,\n",
       " 17162,\n",
       " 1050,\n",
       " 304,\n",
       " 516,\n",
       " 1885,\n",
       " 3269,\n",
       " 24804,\n",
       " 298,\n",
       " 13,\n",
       " 2748,\n",
       " 299,\n",
       " 6036,\n",
       " 1309,\n",
       " 13551,\n",
       " 778,\n",
       " 264,\n",
       " 8613,\n",
       " 369,\n",
       " 10932,\n",
       " 1259,\n",
       " 2655,\n",
       " 10333,\n",
       " 607,\n",
       " 13,\n",
       " 262,\n",
       " 15123,\n",
       " 28725,\n",
       " 478,\n",
       " 1580,\n",
       " 938,\n",
       " 396,\n",
       " 3227,\n",
       " 28742,\n",
       " 28713,\n",
       " 23037,\n",
       " 28725,\n",
       " 304,\n",
       " 6139,\n",
       " 272,\n",
       " 6337,\n",
       " 264,\n",
       " 1664,\n",
       " 13,\n",
       " 28719,\n",
       " 3429,\n",
       " 298,\n",
       " 272,\n",
       " 7635,\n",
       " 1050,\n",
       " 302,\n",
       " 272,\n",
       " 1633,\n",
       " 970,\n",
       " 478,\n",
       " 506,\n",
       " 1432,\n",
       " 2598,\n",
       " 706,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 2486,\n",
       " 369,\n",
       " 1370,\n",
       " 28725,\n",
       " 989,\n",
       " 1683,\n",
       " 654,\n",
       " 17826,\n",
       " 2131,\n",
       " 356,\n",
       " 272,\n",
       " 13283,\n",
       " 302,\n",
       " 264,\n",
       " 1741,\n",
       " 562,\n",
       " 8421,\n",
       " 13,\n",
       " 3888,\n",
       " 28725,\n",
       " 2373,\n",
       " 396,\n",
       " 5115,\n",
       " 28742,\n",
       " 28713,\n",
       " 8123,\n",
       " 302,\n",
       " 272,\n",
       " 2524,\n",
       " 1057,\n",
       " 466,\n",
       " 302,\n",
       " 816,\n",
       " 1754,\n",
       " 28725,\n",
       " 737,\n",
       " 1395,\n",
       " 13,\n",
       " 12938,\n",
       " 4293,\n",
       " 286,\n",
       " 272,\n",
       " 9293,\n",
       " 302,\n",
       " 396,\n",
       " 20311,\n",
       " 1338,\n",
       " 28725,\n",
       " 442,\n",
       " 272,\n",
       " 4431,\n",
       " 302,\n",
       " 741,\n",
       " 13,\n",
       " 7398,\n",
       " 1951,\n",
       " 28723,\n",
       " 415,\n",
       " 9555,\n",
       " 541,\n",
       " 1600,\n",
       " 302,\n",
       " 16748,\n",
       " 6049,\n",
       " 3837,\n",
       " 298,\n",
       " 272,\n",
       " 9829,\n",
       " 302,\n",
       " 13,\n",
       " 1237,\n",
       " 7782,\n",
       " 754,\n",
       " 28716,\n",
       " 6185,\n",
       " 272,\n",
       " 2130,\n",
       " 28725,\n",
       " 304,\n",
       " 10504,\n",
       " 288,\n",
       " 871,\n",
       " 3199,\n",
       " 1868,\n",
       " 395,\n",
       " 264,\n",
       " 13,\n",
       " 450,\n",
       " 7928,\n",
       " 295,\n",
       " 441,\n",
       " 28723,\n",
       " 415,\n",
       " 408,\n",
       " 748,\n",
       " 302,\n",
       " 272,\n",
       " 4376,\n",
       " 654,\n",
       " 5398,\n",
       " 298,\n",
       " 2333,\n",
       " 2108,\n",
       " 27118,\n",
       " 28725,\n",
       " 304,\n",
       " 13,\n",
       " 1237,\n",
       " 14373,\n",
       " 6601,\n",
       " 302,\n",
       " 272,\n",
       " 1370,\n",
       " 403,\n",
       " 2108,\n",
       " 2106,\n",
       " 28725,\n",
       " 390,\n",
       " 272,\n",
       " 5106,\n",
       " 263,\n",
       " 363,\n",
       " 377,\n",
       " 734,\n",
       " 302,\n",
       " 272,\n",
       " 13,\n",
       " 7558,\n",
       " 28713,\n",
       " 304,\n",
       " 285,\n",
       " 696,\n",
       " 1606,\n",
       " 8536,\n",
       " 2747,\n",
       " 652,\n",
       " 15080,\n",
       " 28724,\n",
       " 21137,\n",
       " 28725,\n",
       " 304,\n",
       " 26916,\n",
       " 297,\n",
       " 272,\n",
       " 13,\n",
       " 270,\n",
       " 7186,\n",
       " 8800,\n",
       " 28723,\n",
       " 9054,\n",
       " 369,\n",
       " 14232,\n",
       " 9296,\n",
       " 28725,\n",
       " 690,\n",
       " 14191,\n",
       " 272,\n",
       " 281,\n",
       " 671,\n",
       " 3467,\n",
       " 13,\n",
       " 28713,\n",
       " 353,\n",
       " 434,\n",
       " 1494,\n",
       " 302,\n",
       " 396,\n",
       " 2556,\n",
       " 13894,\n",
       " 297,\n",
       " 4398,\n",
       " 28725,\n",
       " 660,\n",
       " 28728,\n",
       " 8744,\n",
       " 272,\n",
       " 427,\n",
       " 27596,\n",
       " 6297,\n",
       " 28725,\n",
       " 13,\n",
       " 1646,\n",
       " 17488,\n",
       " 865,\n",
       " 486,\n",
       " 272,\n",
       " 2859,\n",
       " 14549,\n",
       " 302,\n",
       " 272,\n",
       " 1683,\n",
       " 28725,\n",
       " 272,\n",
       " 20636,\n",
       " 304,\n",
       " 17898,\n",
       " 13,\n",
       " 28707,\n",
       " 377,\n",
       " 302,\n",
       " 264,\n",
       " 4768,\n",
       " 386,\n",
       " 13160,\n",
       " 28725,\n",
       " 272,\n",
       " 2312,\n",
       " 556,\n",
       " 440,\n",
       " 7843,\n",
       " 302,\n",
       " 741,\n",
       " 319,\n",
       " 5675,\n",
       " 28724,\n",
       " 461,\n",
       " 339,\n",
       " 28725,\n",
       " 442,\n",
       " 264,\n",
       " 1719,\n",
       " 3572,\n",
       " 13,\n",
       " 266,\n",
       " 272,\n",
       " 8120,\n",
       " 28725,\n",
       " 477,\n",
       " 272,\n",
       " 21287,\n",
       " 712,\n",
       " 283,\n",
       " 302,\n",
       " 264,\n",
       " 15569,\n",
       " 2130,\n",
       " 9197,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 18171,\n",
       " 10351,\n",
       " 982,\n",
       " 304,\n",
       " 7098,\n",
       " 7258,\n",
       " 654,\n",
       " 28725,\n",
       " 3545,\n",
       " 28725,\n",
       " 1368,\n",
       " 8228,\n",
       " 298,\n",
       " 272,\n",
       " 13,\n",
       " 994,\n",
       " 12242,\n",
       " 28725,\n",
       " 298,\n",
       " 3924,\n",
       " 652,\n",
       " 4501,\n",
       " 477,\n",
       " 272,\n",
       " 680,\n",
       " 5853,\n",
       " 3209,\n",
       " 302,\n",
       " 13,\n",
       " 1237,\n",
       " 361,\n",
       " 19198,\n",
       " 28723,\n",
       " 4023,\n",
       " 624,\n",
       " 302,\n",
       " 1167,\n",
       " 1448,\n",
       " 1685,\n",
       " 404,\n",
       " 6642,\n",
       " 272,\n",
       " 2760,\n",
       " 4759,\n",
       " 304,\n",
       " 13,\n",
       " 28727,\n",
       " 666,\n",
       " 932,\n",
       " 406,\n",
       " 267,\n",
       " 1339,\n",
       " 302,\n",
       " 264,\n",
       " 8271,\n",
       " 302,\n",
       " 272,\n",
       " 16748,\n",
       " 28725,\n",
       " 272,\n",
       " 799,\n",
       " 8244,\n",
       " 1345,\n",
       " 28725,\n",
       " 13,\n",
       " 14968,\n",
       " 272,\n",
       " 5934,\n",
       " 302,\n",
       " 516,\n",
       " 26705,\n",
       " 304,\n",
       " 5597,\n",
       " 8639,\n",
       " 465,\n",
       " 4731,\n",
       " 1339,\n",
       " 28725,\n",
       " 272,\n",
       " 1170,\n",
       " 8918,\n",
       " 28725,\n",
       " 13,\n",
       " 3128,\n",
       " 4376,\n",
       " 7945,\n",
       " 448,\n",
       " 304,\n",
       " 1043,\n",
       " 28733,\n",
       " 28722,\n",
       " 8744,\n",
       " 4630,\n",
       " 296,\n",
       " 302,\n",
       " 624,\n",
       " 693,\n",
       " 1659,\n",
       " 3452,\n",
       " 24871,\n",
       " 13,\n",
       " 3211,\n",
       " 264,\n",
       " 6392,\n",
       " 2564,\n",
       " 465,\n",
       " 28723,\n",
       " 415,\n",
       " 4494,\n",
       " 403,\n",
       " 24732,\n",
       " 356,\n",
       " 272,\n",
       " 948,\n",
       " 302,\n",
       " 264,\n",
       " 290,\n",
       " 2158,\n",
       " 28724,\n",
       " 13,\n",
       " 1582,\n",
       " 28725,\n",
       " 297,\n",
       " 264,\n",
       " 1704,\n",
       " 482,\n",
       " 369,\n",
       " 15463,\n",
       " 713,\n",
       " 298,\n",
       " 5110,\n",
       " 269,\n",
       " 272,\n",
       " 2030,\n",
       " 302,\n",
       " 516,\n",
       " 13,\n",
       " 644,\n",
       " 28711,\n",
       " 374,\n",
       " 3842,\n",
       " 28725,\n",
       " 486,\n",
       " 272,\n",
       " 10325,\n",
       " 562,\n",
       " 4072,\n",
       " 495,\n",
       " 7165,\n",
       " 1238,\n",
       " 302,\n",
       " 396,\n",
       " 6735,\n",
       " 13,\n",
       " 980,\n",
       " 2569,\n",
       " 297,\n",
       " 12745,\n",
       " 28723,\n",
       " 2354,\n",
       " 2187,\n",
       " 28725,\n",
       " 690,\n",
       " 403,\n",
       " 5597,\n",
       " 15913,\n",
       " 28725,\n",
       " 7567,\n",
       " 264,\n",
       " 13,\n",
       " 360,\n",
       " 16878,\n",
       " 877,\n",
       " 11706,\n",
       " 302,\n",
       " 3168,\n",
       " 28725,\n",
       " 10421,\n",
       " 297,\n",
       " 791,\n",
       " 4082,\n",
       " 1006,\n",
       " 9304,\n",
       " 302,\n",
       " 3075,\n",
       " 304,\n",
       " 13,\n",
       " 10323,\n",
       " 28723,\n",
       " 2354,\n",
       " 11640,\n",
       " 480,\n",
       " 4393,\n",
       " 1335,\n",
       " 28725,\n",
       " 356,\n",
       " 690,\n",
       " 708,\n",
       " 799,\n",
       " 3691,\n",
       " 821,\n",
       " 272,\n",
       " 1162,\n",
       " 13,\n",
       " 4717,\n",
       " 304,\n",
       " 484,\n",
       " 3098,\n",
       " 20162,\n",
       " 10431,\n",
       " 2917,\n",
       " 8582,\n",
       " 632,\n",
       " 28792,\n",
       " 28782,\n",
       " 28793,\n",
       " 403,\n",
       " 23910,\n",
       " 28725,\n",
       " 403,\n",
       " 1671,\n",
       " 13,\n",
       " 1334,\n",
       " 1686,\n",
       " 302,\n",
       " 707,\n",
       " 2112,\n",
       " 28725,\n",
       " 395,\n",
       " 272,\n",
       " 5851,\n",
       " 302,\n",
       " 264,\n",
       " 2128,\n",
       " 11969,\n",
       " 317,\n",
       " 17968,\n",
       " 28742,\n",
       " 28713,\n",
       " 549,\n",
       " 2150,\n",
       " 28725,\n",
       " 13,\n",
       " 6087,\n",
       " 12554,\n",
       " 516,\n",
       " 22718,\n",
       " 28725,\n",
       " 304,\n",
       " 3289,\n",
       " 286,\n",
       " 754,\n",
       " 272,\n",
       " 1749,\n",
       " 7793,\n",
       " 28723,\n",
       " 330,\n",
       " 298,\n",
       " 705,\n",
       " 28716,\n",
       " 22527,\n",
       " 13,\n",
       " 391,\n",
       " 10431,\n",
       " 2917,\n",
       " 28733,\n",
       " 13223,\n",
       " 1027,\n",
       " 28725,\n",
       " 302,\n",
       " 4300,\n",
       " 6298,\n",
       " 482,\n",
       " 28725,\n",
       " 654,\n",
       " 297,\n",
       " 516,\n",
       " 319,\n",
       " 1844,\n",
       " 291,\n",
       " 28745,\n",
       " 1312,\n",
       " 264,\n",
       " 13,\n",
       " 10046,\n",
       " 5469,\n",
       " 19427,\n",
       " 28725,\n",
       " 302,\n",
       " 369,\n",
       " 3127,\n",
       " 395,\n",
       " 690,\n",
       " 272,\n",
       " 4920,\n",
       " 302,\n",
       " 272,\n",
       " 19719,\n",
       " 13,\n",
       " 22508,\n",
       " 652,\n",
       " 8639,\n",
       " 465,\n",
       " 23180,\n",
       " 28725,\n",
       " 4897,\n",
       " 1656,\n",
       " 12298,\n",
       " 2673,\n",
       " 516,\n",
       " 13034,\n",
       " 304,\n",
       " 268,\n",
       " 473,\n",
       " 14589,\n",
       " 13,\n",
       " 28729,\n",
       " 485,\n",
       " 28706,\n",
       " 28723,\n",
       " 415,\n",
       " 14708,\n",
       " 8118,\n",
       " 28725,\n",
       " 2173,\n",
       " 8723,\n",
       " 2132,\n",
       " 1816,\n",
       " 28725,\n",
       " 304,\n",
       " 14272,\n",
       " 2113,\n",
       " 269,\n",
       " 617,\n",
       " 302,\n",
       " 13,\n",
       " 894,\n",
       " 25838,\n",
       " 28725,\n",
       " 682,\n",
       " 14543,\n",
       " 369,\n",
       " 400,\n",
       " 553,\n",
       " 5048,\n",
       " 272,\n",
       " 15068,\n",
       " 271,\n",
       " 302,\n",
       " 516,\n",
       " 2202,\n",
       " 28725,\n",
       " 13,\n",
       " 3128,\n",
       " 708,\n",
       " 12380,\n",
       " 302,\n",
       " 15224,\n",
       " 6178,\n",
       " 298,\n",
       " 506,\n",
       " 2783,\n",
       " 6334,\n",
       " 2106,\n",
       " 516,\n",
       " 676,\n",
       " 5079,\n",
       " 28723,\n",
       " 13,\n",
       " 13,\n",
       " 1014,\n",
       " 4108,\n",
       " 302,\n",
       " 272,\n",
       " 3075,\n",
       " 676,\n",
       " 28725,\n",
       " 4612,\n",
       " 3080,\n",
       " 486,\n",
       " 1259,\n",
       " 5099,\n",
       " 390,\n",
       " 654,\n",
       " 459,\n",
       " 13270,\n",
       " 4742,\n",
       " 13,\n",
       " 1403,\n",
       " 516,\n",
       " 8155,\n",
       " 28725,\n",
       " 403,\n",
       " 737,\n",
       " 369,\n",
       " 302,\n",
       " 624,\n",
       " 693,\n",
       " 553,\n",
       " 2651,\n",
       " 1856,\n",
       " 14498,\n",
       " 304,\n",
       " 13,\n",
       " 720,\n",
       " 930,\n",
       " 296,\n",
       " 477,\n",
       " 516,\n",
       " 21864,\n",
       " 9539,\n",
       " 28723,\n",
       " 2354,\n",
       " 1338,\n",
       " 28725,\n",
       " 2070,\n",
       " 2009,\n",
       " 12723,\n",
       " 28725,\n",
       " 403,\n",
       " 13,\n",
       " 28712,\n",
       " 1223,\n",
       " 998,\n",
       " 4265,\n",
       " 601,\n",
       " 821,\n",
       " 2173,\n",
       " 28745,\n",
       " 562,\n",
       " 1012,\n",
       " 23284,\n",
       " 304,\n",
       " 14540,\n",
       " 6178,\n",
       " 1117,\n",
       " 969,\n",
       " 13,\n",
       " 391,\n",
       " 1176,\n",
       " 324,\n",
       " 601,\n",
       " 486,\n",
       " 521,\n",
       " 1569,\n",
       " 3309,\n",
       " 15368,\n",
       " 304,\n",
       " 298,\n",
       " 309,\n",
       " 28723,\n",
       " 650,\n",
       " 12003,\n",
       " 264,\n",
       " 16268,\n",
       " 28733,\n",
       " 21122,\n",
       " 13,\n",
       " 1009,\n",
       " 8613,\n",
       " 5344,\n",
       " 28725,\n",
       " 285,\n",
       " 699,\n",
       " 286,\n",
       " 395,\n",
       " 23716,\n",
       " 9684,\n",
       " 28792,\n",
       " 28784,\n",
       " 1181,\n",
       " 304,\n",
       " 264,\n",
       " 5561,\n",
       " 2058,\n",
       " 302,\n",
       " 1321,\n",
       " 1126,\n",
       " 13,\n",
       " 6974,\n",
       " 553,\n",
       " 750,\n",
       " 480,\n",
       " 1334,\n",
       " 302,\n",
       " 652,\n",
       " 2982,\n",
       " 28723,\n",
       " 650,\n",
       " 835,\n",
       " 22809,\n",
       " 264,\n",
       " 12579,\n",
       " 297,\n",
       " 264,\n",
       " 319,\n",
       " 1844,\n",
       " 291,\n",
       " 302,\n",
       " 13,\n",
       " 28727,\n",
       " 1057,\n",
       " 383,\n",
       " 28725,\n",
       " 737,\n",
       " 369,\n",
       " 690,\n",
       " 1885,\n",
       " 1311,\n",
       " 272,\n",
       " 752,\n",
       " 440,\n",
       " 28724,\n",
       " 5925,\n",
       " 1339,\n",
       " 302,\n",
       " 272,\n",
       " 6735,\n",
       " 28725,\n",
       " 562,\n",
       " 13,\n",
       " 1510,\n",
       " 298,\n",
       " 705,\n",
       " 28716,\n",
       " 22527,\n",
       " 28723,\n",
       " 2354,\n",
       " 290,\n",
       " 402,\n",
       " 11768,\n",
       " 1126,\n",
       " 654,\n",
       " 442,\n",
       " 10059,\n",
       " 286,\n",
       " 1024,\n",
       " 272,\n",
       " 11997,\n",
       " 8844,\n",
       " 302,\n",
       " 272,\n",
       " 13,\n",
       " 28711,\n",
       " 5087,\n",
       " 28725,\n",
       " 1312,\n",
       " 272,\n",
       " 865,\n",
       " 744,\n",
       " 302,\n",
       " 516,\n",
       " 916,\n",
       " 28733,\n",
       " 28715,\n",
       " 638,\n",
       " 690,\n",
       " 6178,\n",
       " 3624,\n",
       " 272,\n",
       " 13,\n",
       " 28716,\n",
       " 20128,\n",
       " 28733,\n",
       " 28722,\n",
       " 16013,\n",
       " 28725,\n",
       " 403,\n",
       " 264,\n",
       " 5964,\n",
       " 302,\n",
       " 9918,\n",
       " 24208,\n",
       " 462,\n",
       " 1576,\n",
       " 742,\n",
       " 28725,\n",
       " 369,\n",
       " 305,\n",
       " 2701,\n",
       " 438,\n",
       " 272,\n",
       " 8480,\n",
       " 28725,\n",
       " 13,\n",
       " 391,\n",
       " 690,\n",
       " 654,\n",
       " 319,\n",
       " 4136,\n",
       " 286,\n",
       " 2747,\n",
       " 272,\n",
       " 13431,\n",
       " 395,\n",
       " 272,\n",
       " 268,\n",
       " 473,\n",
       " 6420,\n",
       " 302]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "len(encoded_train_dataset['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:03.247022Z",
     "start_time": "2024-05-05T09:33:03.238088Z"
    }
   },
   "id": "45372df9a4c8429e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_val_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:04.034900Z",
     "start_time": "2024-05-05T09:33:04.030213Z"
    }
   },
   "id": "bcc5687c71876ce3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "encoded_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:05.167906Z",
     "start_time": "2024-05-05T09:33:05.163460Z"
    }
   },
   "id": "da0f652ddcdf131d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "# encoded_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:06.208658Z",
     "start_time": "2024-05-05T09:33:06.206240Z"
    }
   },
   "id": "606630419739dc2b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "'''training_args = TrainingArguments(output_dir=\"test_trainer\",                                   \n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                 num_train_epochs=5,\n",
    "                                  learning_rate=0.001,\n",
    "                                  optim='adamw_torch'\n",
    "                                   )'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:06.780172Z",
     "start_time": "2024-05-05T09:33:06.776079Z"
    }
   },
   "id": "6409409bf6ac81a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_args = TrainingArguments(output_dir=\"test_trainer\",                                   \\n                                  per_device_train_batch_size=1,\\n                                  per_device_eval_batch_size=1,\\n                                 num_train_epochs=5,\\n                                  learning_rate=0.001,\\n                                  optim=\\'adamw_torch\\'\\n                                   )'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=0.001,\n",
    "                                  logging_steps=1,\n",
    "                                  fp16=True\n",
    "                                  # max_steps=1\n",
    "                                  )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:07.616631Z",
     "start_time": "2024-05-05T09:33:07.568523Z"
    }
   },
   "id": "f6bf9f149dd3ca96",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:11.596495Z",
     "start_time": "2024-05-05T09:33:09.046083Z"
    }
   },
   "id": "4b2da725494d317a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "model.add_adapter(peft_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:12.559736Z",
     "start_time": "2024-05-05T09:33:12.154011Z"
    }
   },
   "id": "705596b4ab2ec935",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "How to Use Rouge\n",
    "At minimum, this metric takes as input a list of predictions and a list of references:\n",
    "\n",
    ">>> rouge = evaluate.load('rouge')\n",
    ">>> predictions = [\"hello there\", \"general kenobi\"]\n",
    ">>> references = [\"hello there\", \"general kenobi\"]\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references)\n",
    ">>> print(results)\n",
    "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
    "One can also pass a custom tokenizer which is especially useful for non-latin languages.\n",
    "\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references,\n",
    "                            tokenizer=lambda x: x.split())\n",
    ">>> print(results)\n",
    "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
    "It can also deal with lists of references for each predictions:\n",
    "\n",
    ">>> rouge = evaluate.load('rouge')\n",
    ">>> predictions = [\"hello there\", \"general kenobi\"]\n",
    ">>> references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references)\n",
    ">>> print(results)\n",
    "{'rouge1': 0.8333, 'rouge2': 0.5, 'rougeL': 0.8333, 'rougeLsum': 0.8333}```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ef91ae2fe690d73"
  },
  {
   "cell_type": "code",
   "source": [
    "print_number_of_trainable_model_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:17.633843Z",
     "start_time": "2024-05-05T09:33:17.625269Z"
    }
   },
   "id": "38741509524fc3ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 20971520\\nall model parameters: 3773042688\\npercentage of trainable model parameters: 0.56%'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_val_dataset,\n",
    "    # compute_metrics=rouge\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:33:27.190284Z",
     "start_time": "2024-05-05T09:33:27.163780Z"
    }
   },
   "id": "7586a4126c2f95c7",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3f90d0cd8545ae07"
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:59:34.629087Z",
     "start_time": "2024-05-05T09:33:33.857325Z"
    }
   },
   "id": "6366767320253bfc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 25:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>17.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>12.888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>11.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>12.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>11.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>9.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>8.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>13.851600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>13.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.721600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>11.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>9.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>9.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>8.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>8.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>9.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>7.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>8.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>8.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>7.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>6.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>5.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>8.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>8.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>7.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>8.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>20.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>24.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>18.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>21.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>7.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>8.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>6.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.537400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=10.549319248199463, metrics={'train_runtime': 1560.628, 'train_samples_per_second': 0.032, 'train_steps_per_second': 0.032, 'total_flos': 2139489484800000.0, 'train_loss': 10.549319248199463, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "trainer.save_model('models/mistral_model_v1')",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T10:11:12.774204Z",
     "start_time": "2024-05-05T10:11:12.308965Z"
    }
   },
   "id": "4bf017f90b6cb5eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yferc\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\transformers\\integrations\\peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(encoded_train_dataset['attention_mask'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:23:26.439556Z",
     "start_time": "2024-05-03T15:23:25.929001Z"
    }
   },
   "id": "943bbffbc30f0e5e",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column attention_mask not in the dataset. Current columns in the dataset: ['input_ids', 'labels']\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(encoded_train_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[0;32m   2860\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2861\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem(key)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\arrow_dataset.py:2845\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[1;34m(self, key, **kwargs)\u001B[0m\n\u001B[0;32m   2843\u001B[0m format_kwargs \u001B[38;5;241m=\u001B[39m format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m   2844\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[1;32m-> 2845\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m query_table(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data, key, indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices)\n\u001B[0;32m   2846\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m format_table(\n\u001B[0;32m   2847\u001B[0m     pa_subtable, key, formatter\u001B[38;5;241m=\u001B[39mformatter, format_columns\u001B[38;5;241m=\u001B[39mformat_columns, output_all_columns\u001B[38;5;241m=\u001B[39moutput_all_columns\n\u001B[0;32m   2848\u001B[0m )\n\u001B[0;32m   2849\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\formatting\\formatting.py:584\u001B[0m, in \u001B[0;36mquery_table\u001B[1;34m(table, key, indices)\u001B[0m\n\u001B[0;32m    582\u001B[0m         _raise_bad_key_type(key)\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 584\u001B[0m     _check_valid_column_key(key, table\u001B[38;5;241m.\u001B[39mcolumn_names)\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    586\u001B[0m     size \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mnum_rows \u001B[38;5;28;01mif\u001B[39;00m indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m table\u001B[38;5;241m.\u001B[39mnum_rows\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\formatting\\formatting.py:521\u001B[0m, in \u001B[0;36m_check_valid_column_key\u001B[1;34m(key, columns)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_valid_column_key\u001B[39m(key: \u001B[38;5;28mstr\u001B[39m, columns: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[1;32m--> 521\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. Current columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Column attention_mask not in the dataset. Current columns in the dataset: ['input_ids', 'labels']\""
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(encoded_train_dataset['attention_mask'][78]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:23:29.217006Z",
     "start_time": "2024-05-03T15:23:29.183265Z"
    }
   },
   "id": "29712cac9bb0dcba",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column attention_mask not in the dataset. Current columns in the dataset: ['input_ids', 'labels']\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(encoded_train_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m78\u001B[39m]))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[0;32m   2860\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2861\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem(key)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\arrow_dataset.py:2845\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[1;34m(self, key, **kwargs)\u001B[0m\n\u001B[0;32m   2843\u001B[0m format_kwargs \u001B[38;5;241m=\u001B[39m format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m   2844\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[1;32m-> 2845\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m query_table(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data, key, indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices)\n\u001B[0;32m   2846\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m format_table(\n\u001B[0;32m   2847\u001B[0m     pa_subtable, key, formatter\u001B[38;5;241m=\u001B[39mformatter, format_columns\u001B[38;5;241m=\u001B[39mformat_columns, output_all_columns\u001B[38;5;241m=\u001B[39moutput_all_columns\n\u001B[0;32m   2848\u001B[0m )\n\u001B[0;32m   2849\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\formatting\\formatting.py:584\u001B[0m, in \u001B[0;36mquery_table\u001B[1;34m(table, key, indices)\u001B[0m\n\u001B[0;32m    582\u001B[0m         _raise_bad_key_type(key)\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 584\u001B[0m     _check_valid_column_key(key, table\u001B[38;5;241m.\u001B[39mcolumn_names)\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    586\u001B[0m     size \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mnum_rows \u001B[38;5;28;01mif\u001B[39;00m indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m table\u001B[38;5;241m.\u001B[39mnum_rows\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\projet-annuel\\Lib\\site-packages\\datasets\\formatting\\formatting.py:521\u001B[0m, in \u001B[0;36m_check_valid_column_key\u001B[1;34m(key, columns)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_valid_column_key\u001B[39m(key: \u001B[38;5;28mstr\u001B[39m, columns: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[1;32m--> 521\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. Current columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Column attention_mask not in the dataset. Current columns in the dataset: ['input_ids', 'labels']\""
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "80221280ffa61a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(encoded_train_dataset['input_ids'])):\n",
    "    if len(encoded_train_dataset['input_ids'][i]) == 11593:\n",
    "        print(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83dd58e6b4acf72c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b2616ae4e0788344",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myJupyterKernel",
   "language": "python",
   "name": "myjupyterkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
