{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:35:21.635847500Z",
     "start_time": "2024-04-18T20:35:18.083197600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=\"float16\",\n",
    "#         \n",
    "# )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:02:34.414343900Z",
     "start_time": "2024-04-15T21:02:34.406737500Z"
    }
   },
   "id": "41c861dc4f0b4aa0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "        # llm_int8_has_fp16_weight=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:36:07.136677500Z",
     "start_time": "2024-04-18T20:36:07.127669700Z"
    }
   },
   "id": "c6e244d72923a929"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:35:22.014567200Z",
     "start_time": "2024-04-18T20:35:22.009560100Z"
    }
   },
   "id": "e9e6cdf6057587fa"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e2b58ec15fd4c3282159afedbf12379"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=quantization_config, device_map=\"auto\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:36:29.543923600Z",
     "start_time": "2024-04-18T20:36:09.817010400Z"
    }
   },
   "id": "e33c2b046ed4716f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "9785851904"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:37:15.755617900Z",
     "start_time": "2024-04-18T20:37:15.749814Z"
    }
   },
   "id": "978de3100f5efe53"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:37:18.393312600Z",
     "start_time": "2024-04-18T20:37:18.005825700Z"
    }
   },
   "id": "964f31d4820fa92b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "prompt = \"what's the capital of france\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:37:18.399716700Z",
     "start_time": "2024-04-18T20:37:18.395202400Z"
    }
   },
   "id": "2f7bd92d37ff6be1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:37:19.059410700Z",
     "start_time": "2024-04-18T20:37:19.032018700Z"
    }
   },
   "id": "d1804bedd42caf2d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "E:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"<s> what's the capital of france? Paris France France France France France France France France France France France France France France France France France France France France</s>\""
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T20:38:11.032259500Z",
     "start_time": "2024-04-18T20:37:20.844158400Z"
    }
   },
   "id": "2e2485b7a5476522"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:26:07.726892900Z",
     "start_time": "2024-04-15T20:26:07.601570800Z"
    }
   },
   "id": "34442764273803f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:26:07.758990800Z",
     "start_time": "2024-04-15T20:26:07.605242500Z"
    }
   },
   "id": "8532935d1b9b517d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\accelerate\\big_modeling.py:456\u001B[0m, in \u001B[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m param\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    455\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\cours\\PA\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2554\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2550\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule\u001B[38;5;241m.\u001B[39mto)\n\u001B[0;32m   2551\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   2552\u001B[0m     \u001B[38;5;66;03m# Checks if the model has been loaded in 8-bit\u001B[39;00m\n\u001B[0;32m   2553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES:\n\u001B[1;32m-> 2554\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2555\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2556\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2557\u001B[0m         )\n\u001B[0;32m   2558\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mGPTQ:\n\u001B[0;32m   2559\u001B[0m         \u001B[38;5;66;03m# For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\u001B[39;00m\n\u001B[0;32m   2560\u001B[0m         \u001B[38;5;66;03m# the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\u001B[39;00m\n\u001B[0;32m   2561\u001B[0m         dtype_present_in_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "# model.to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:52:52.877428300Z",
     "start_time": "2024-04-15T20:52:52.793295300Z"
    }
   },
   "id": "62b0cb4ff5a09f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:53:39.881153500Z",
     "start_time": "2024-04-15T20:53:39.801201500Z"
    }
   },
   "id": "21b0a1e1a0b6e9a4"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'<s> what\\'s 4 + 10 ?\\n\\n```\\n1. \\tdef increment(inputNum: Int) = inputNum + 4 // this is just for example purposes; it\\'s wrong.\\n\\t\\t                                        // you will have to change it to return correctly.\\n2.\\tval myVal: Int = (increment(\"0\"))\\n```\\n\\nIn order to answer this question, I will show you the various types of functions in both Scala and Java. This is'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:59:01.200362100Z",
     "start_time": "2024-04-15T20:58:53.894959300Z"
    }
   },
   "id": "dce133f74b316ae4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "60c3b227ee057524"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
