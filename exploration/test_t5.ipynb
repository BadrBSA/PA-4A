{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de364d1e629541218d08573ef058353d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def split_text(text, max_tokens=512, overlap=50):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    segments = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_tokens\n",
    "        segment = tokens[start:end]\n",
    "        segments.append(segment)\n",
    "        start += max_tokens - overlap  # Avancer avec chevauchement\n",
    "\n",
    "    return segments\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def summarize_segment(segment_tokens):\n",
    "    segment_text = tokenizer.decode(segment_tokens, skip_special_tokens=True)\n",
    "    inputs = tokenizer.encode(\"make a short summary of the following text: \" + segment_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs, max_length=513, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def hierarchical_summarize(text, max_segment_length=513, overlap=50, max_iterations=10):\n",
    "    segments = split_text(text, max_tokens=max_segment_length, overlap=overlap)\n",
    "    summaries = []\n",
    "\n",
    "    for segment in segments:\n",
    "        summary = summarize_segment(segment)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    combined_summary = \" \".join(summaries)\n",
    "\n",
    "    for i in range(max_iterations - 1):\n",
    "        print(i)\n",
    "        if len(tokenizer.encode(combined_summary)) <= max_segment_length:\n",
    "            break\n",
    "\n",
    "        segments = split_text(combined_summary, max_tokens=max_segment_length, overlap=overlap)\n",
    "        summaries = []\n",
    "\n",
    "        for segment in segments:\n",
    "            summary = summarize_segment(segment)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        combined_summary = \" \".join(summaries)\n",
    "        i+=1\n",
    "\n",
    "    return combined_summary\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"pszemraj/booksum-short\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "text = dataset['train']['chapter'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "C:\\Users\\devar\\Desktop\\ESGI\\master\\M1\\S2\\PA-4A\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m final_summary \u001B[38;5;241m=\u001B[39m \u001B[43mhierarchical_summarize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[20], line 6\u001B[0m, in \u001B[0;36mhierarchical_summarize\u001B[1;34m(text, max_segment_length, overlap, max_iterations)\u001B[0m\n\u001B[0;32m      3\u001B[0m summaries \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m segment \u001B[38;5;129;01min\u001B[39;00m segments:\n\u001B[1;32m----> 6\u001B[0m     summary \u001B[38;5;241m=\u001B[39m \u001B[43msummarize_segment\u001B[49m\u001B[43m(\u001B[49m\u001B[43msegment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     summaries\u001B[38;5;241m.\u001B[39mappend(summary)\n\u001B[0;32m      9\u001B[0m combined_summary \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(summaries)\n",
      "Cell \u001B[1;32mIn[19], line 4\u001B[0m, in \u001B[0;36msummarize_segment\u001B[1;34m(segment_tokens)\u001B[0m\n\u001B[0;32m      2\u001B[0m segment_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(segment_tokens, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake a short summary of the following text: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m segment_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 4\u001B[0m summary_ids \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m513\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m summary \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(summary_ids[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m summary\n",
      "File \u001B[1;32m~\\Desktop\\ESGI\\master\\M1\\S2\\PA-4A\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\ESGI\\master\\M1\\S2\\PA-4A\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1609\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1602\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   1603\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1604\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   1605\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   1606\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1607\u001B[0m     )\n\u001B[0;32m   1608\u001B[0m     \u001B[38;5;66;03m# 13. run beam search\u001B[39;00m\n\u001B[1;32m-> 1609\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1610\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1615\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1616\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1617\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_logits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1618\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1619\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1620\u001B[0m \u001B[43m        \u001B[49m\u001B[43msequential\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1621\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1622\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1624\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE:\n\u001B[0;32m   1625\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[0;32m   1626\u001B[0m     logits_warper \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config)\n",
      "File \u001B[1;32m~\\Desktop\\ESGI\\master\\M1\\S2\\PA-4A\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3118\u001B[0m, in \u001B[0;36mGenerationMixin._beam_search\u001B[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001B[0m\n\u001B[0;32m   3115\u001B[0m next_tokens \u001B[38;5;241m=\u001B[39m next_tokens \u001B[38;5;241m%\u001B[39m vocab_size\n\u001B[0;32m   3117\u001B[0m \u001B[38;5;66;03m# stateless\u001B[39;00m\n\u001B[1;32m-> 3118\u001B[0m beam_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mbeam_scorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3119\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_token_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3124\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeam_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3126\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3127\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3129\u001B[0m beam_scores \u001B[38;5;241m=\u001B[39m beam_outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_beam_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   3130\u001B[0m beam_next_tokens \u001B[38;5;241m=\u001B[39m beam_outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_beam_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\ESGI\\master\\M1\\S2\\PA-4A\\venv\\Lib\\site-packages\\transformers\\generation\\beam_search.py:267\u001B[0m, in \u001B[0;36mBeamSearchScorer.process\u001B[1;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001B[0m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# next tokens for this sentence\u001B[39;00m\n\u001B[0;32m    265\u001B[0m beam_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m beam_token_rank, (next_token, next_score, next_index) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\n\u001B[1;32m--> 267\u001B[0m     \u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnext_tokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_scores\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_indices\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m ):\n\u001B[0;32m    269\u001B[0m     batch_beam_idx \u001B[38;5;241m=\u001B[39m batch_idx \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_size \u001B[38;5;241m+\u001B[39m next_index\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;66;03m# add to generated hypotheses if end of sentence\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\ESGI\\master\\M1\\S2\\PA-4A\\venv\\Lib\\site-packages\\torch\\_tensor.py:1037\u001B[0m, in \u001B[0;36mTensor.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1027\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1028\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1029\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRecommended usage would be tensor.shape[0]. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1033\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   1034\u001B[0m         )\n\u001B[0;32m   1035\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m-> 1037\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1038\u001B[0m     \u001B[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m     \u001B[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m     \u001B[38;5;66;03m# save us work, and also helps keep trace ordering deterministic\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;66;03m# (e.g., if you zip(*hiddens), the eager map will force all the\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m     \u001B[38;5;66;03m# indexes of hiddens[0] before hiddens[1], while the generator\u001B[39;00m\n\u001B[0;32m   1043\u001B[0m     \u001B[38;5;66;03m# map will interleave them.)\u001B[39;00m\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001B[39;00m\n\u001B[0;32m   1045\u001B[0m     \u001B[38;5;66;03m# See gh-54457\u001B[39;00m\n\u001B[0;32m   1046\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration over a 0-d tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "final_summary = hierarchical_summarize(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heyward's letter to his friend, James Webb, in which he describes his encounter with a group of prisoners in a remote part of North Carolina, in the summer of 1668, can be found on the website of the University of North Carolina at Greensboro. In our series of letters from African-American journalists, film-maker and columnist Richard Roeper looks at one of the most famous lines in the history of African-American literature, Langston Hughes' poem, \"I Have a Dream\", which was published in Langston Hughes's posthumously published work, \"I Have a Dream: The Langston Hughes Story\".\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "with open(\"data/books/J._K._Rowling_-_Harry_Potter_1_-_Sorcerers_Stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    long_text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_summary = hierarchical_summarize(long_text)\n",
    "print(final_summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
